{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "10-different-spoken-digit-cnn.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZM2s9V0NQTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip spoken-digit-dataset.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "-zN4BG_JL4VB",
        "colab_type": "code",
        "outputId": "11a97f2e-37c4-4a47-b21d-1f57556c7293",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import librosa\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "print(os.listdir(r\"free-spoken-digit-dataset-master\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['recordings', 'metadata.py', 'pip_requirements.txt', 'utils', 'README.md', 'acquire_data', 'free-spoken-digit-dataset-master', '__init__.py', '.gitignore']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRvSeGKBL4Wq",
        "colab_type": "code",
        "outputId": "c94cc3cd-c6dd-4030-ea14-70fba809d055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "from tqdm import tqdm\n",
        "\n",
        "def wav2mfcc(file_path, augment = False, max_pad_len=11):\n",
        "    wave, sr = librosa.load(file_path, mono=True, sr=8000, duration = 1.024)\n",
        "    \n",
        "    if augment == True:\n",
        "        bins_per_octave = 12\n",
        "        pitch_pm = 4\n",
        "        pitch_change =  pitch_pm * 2*(np.random.uniform())   \n",
        "        wave = librosa.effects.pitch_shift(wave, \n",
        "                                          8000, n_steps=pitch_change, \n",
        "                                          bins_per_octave=bins_per_octave)\n",
        "        \n",
        "        speed_change = np.random.uniform(low=0.9,high=1.1)\n",
        "        wave = librosa.effects.time_stretch(wave, speed_change)\n",
        "        wave = wave[:8192]\n",
        "\n",
        "    duration = wave.shape[0]/sr\n",
        "    speed_change = 2.0* duration/1.024\n",
        "    wave = librosa.effects.time_stretch(wave, speed_change)\n",
        "    wave = wave[:4096]\n",
        "    \n",
        "    wave = librosa.util.normalize(wave)\n",
        "    mfcc = librosa.feature.mfcc(wave, sr=sr, n_mfcc=40, hop_length=int(0.048*sr), n_fft=int(0.096*sr))\n",
        "    mfcc -= (np.mean(mfcc, axis=0) + 1e-8)\n",
        "    #print(\"shape=\",mfcc.shape[1], wave.shape[0])\n",
        "    pad_width = max_pad_len - mfcc.shape[1]\n",
        "    mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "    #mfcc = mfcc[2:24,:]\n",
        "    return mfcc, duration, sr\n",
        "\n",
        "def get_data(dir = '', augment= False):\n",
        "    labels = []\n",
        "    mfccs = []\n",
        "    durations = []\n",
        "    sampling_rates = []\n",
        "    filenames = []\n",
        "\n",
        "    for f in tqdm(os.listdir(dir)):\n",
        "        if f.endswith('.wav'):\n",
        "            mfcc, duration, sr = wav2mfcc(dir + \"/\" + f, augment)\n",
        "            mfccs.append(mfcc)\n",
        "            durations.append(duration)\n",
        "            sampling_rates.append(sr)\n",
        "            # List of labels\n",
        "            label = f.split('_')[0]\n",
        "            labels.append(label)\n",
        "            filenames.append(dir + \"/\" + f)\n",
        "    return filenames, np.asarray(mfccs), np.asarray(durations), np.asarray(sampling_rates), to_categorical(labels), labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4JIkWsFL4XW",
        "colab_type": "code",
        "outputId": "d317cbe4-1abf-4f4b-d8ea-d5ca75241aa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "filenames, mfccs, durations, sampling_rates, labels, cls_true = get_data('free-spoken-digit-dataset-master/free-spoken-digit-dataset-master/recordings')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1500/1500 [03:28<00:00,  7.15it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_rQmza6L4X1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "def plot_images(images, cls_true, cls_pred=None):\n",
        "    assert len(images) == len(cls_true) == 9\n",
        "    \n",
        "    # Create figure with 3x3 sub-plots.\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(15,15))\n",
        "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "    \n",
        "    max = np.max(images)\n",
        "    min = np.min(images)\n",
        "\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        # Plot image.\n",
        "        #ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
        "        im = librosa.display.specshow(images[i], ax=ax, vmin=min, vmax=max)\n",
        "\n",
        "        # Show true and predicted classes.\n",
        "        if cls_pred is None:\n",
        "            xlabel = \"True: {0}\".format(cls_true[i])\n",
        "        else:\n",
        "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
        "\n",
        "        ax.set_xlabel(xlabel)\n",
        "    # Ensure the plot is shown correctly with multiple plots\n",
        "    # in a single Notebook cell.\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F75ghzXL4YX",
        "colab_type": "code",
        "outputId": "4a1b71a9-ec8f-4f76-982c-6a5fe1218007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        }
      },
      "source": [
        "plot_images(mfccs[100:109], cls_true[100:109])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1YAAANLCAYAAABCFAwuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdbahlWXof9mftc899rarunq6eHs1I\nPZaxLEURQSZGEviDcHAw0RdjDMZJINjBjmwUK4QQHAKGQYltcAwCfXKMwTLYECSiGJGEGNk4ijWy\npEwyY2GNNBq9tTSjaY1m+rWq7r3n3LNXPnRVp1HdutNH63lu1an6/WCga6b1v/ucs/fa+7mn1l+t\n9x4AAAD8/k2P+wAAAAB2ncEKAABgkMEKAABgkMEKAABgkMEKAABg0N42//LtWyf94y+9kPfTKwoJ\nWysI3SEVr78is6KNsiKz6Hzq85ye+elf/+JXeu8vpQfvuHfXrQ+lZlZ8fiV6wXG2gt/HVbyfU8W1\nW7MetGf4vvX/WrcudfvmSX/l9vO5oQXXWcVaWLK+zgXPByVrTMS03OrR/IOpWLd35dmwZNiI+PRv\nfOnStWurT+/jL70Qn/yb3593VBUn+v5+fmZE0Y2/4kQvyKx4Tzeb/Mzzs/zMvWV+ZkT0s3vpmcf/\n8X/7anroU+DjL30oPvm3/ovUzPk0//Or0Fer9MxWsB7Mp6fpmdPhYXpmWyzSMyMiWtE6swuO/qP/\nxrp1iVduPx+f/IG/kpq5uXs3NS8iYj47T8+8uJN/nJuz/LVw2q+5bg+/7sPpmRXrYVSshwXPhr3i\neTMiTv7CJy5du/xVQAAAgEEGKwAAgEEGKwAAgEEGKwAAgEEGKwAAgEFbdjq23Ja0ikaRghasiKip\nlaxoGrwoeP0X6/zMis++olq64rVHRKtob+RSvc/p7XjznTupeRERi+eSq5UjIpb5zattmd+EtStX\nQzsoaNaK2J3/NyEXF4/7CJ4draXfJxcnJ6l5ERGLGzfTM/eefy498+L1N9IzK9bCiKJ7QVEzXrq9\nHWnLvsKu3M8AAACeWAYrAACAQQYrAACAQQYrAACAQVuVV/R5jvnOO2k/fCrY9FimYtPuVLBhuSKz\nYtNjRdFExQbFiuOMiDg6qsnlYXOPvjrPjTzPL4lZVFy7BaUIFcUr7bCoFGJXLLbskfogKop39gqO\nk8ttNjG/k1uS0w4PUvMiItoyv4hqKrg/Lm7mXw/T0XF6ZkTUPHcU3F+yS6GqXPdx+sYKAABgkMEK\nAABgkMEKAABgkMEKAABgkMEKAABg0HYVP61FWy6LDiVJQQtWREQs8ptvdkdBK+Dc8zN36SNaFzR2\n8Wg993ybDvZT8yIiLr76enpmq2gaPMm/dtveE35fua/t0n2gpHl1h17/rmst2jK3hfGJf34rtLh1\nKz90R9atiIiY89eDdpDf5toL2kwrjvMqvrECAAAYZLACAAAYZLACAAAYZLACAAAYZLACAAAYlFs5\ns62C9o+oav/YFDTjVah4TyuaxSradCo+o6moBWvyO41rM7X0VqCpoB3u/IuvpWfu3ThOz+wVjZYF\nba7TUf5r36n7y+YiP/OiIJNH6NGTz4tW0cZb8HyQ/bojiprhqlpCV6v8zB1pNG0VbabX/NI93QEA\nAAwyWAEAAAwyWAEAAAwyWAEAAAzasryiR2RufmwFGykrNmdG1Gz8q9iwXFEKsSsqPqOq93N1XpPL\nQ9q0iHZ8kptZUBKzfP5meuZ0kvu6IyJaxXXWCn7HtyObtSNid461F91fedi0yL9+K56PCjJL1pgK\nVaVmFQUOq/xj7QXH2QruBfP5WXrmVXxjBQAAMMhgBQAAMMhgBQAAMMhgBQAAMMhgBQAAMGjLVsBk\nFc0vFW0qEREV5S9zwbEuCj7Sive0qk0nW8VnFKFd6zr1HrG5yM0sWLsWL7yQnrkzjV0Hh/mZBc2N\nZetWRftowevvFe8pl+s9/3wruHYrWtxiavmZFffyqvt4wXva+25cu71gjZ0q7i9X/bxr/WkAAABP\nIYMVAADAIIMVAADAIIMVAADAIIMVAADAoK0q5Fq03EaotiPNLxE17S8Vr399np9ZoKL5pVU0a1Wp\n+Ox5tCn5d0gF60w7PE7PLGnGq7jOKppHS+4vRa2Au/L6uT4t0tvh+mqVmhcR0QvOs+n4JD2z4hmu\nrCWz4PmoV9yzlrvxzDWvrve52DdWAAAAgwxWAAAAgwxWAAAAgwxWAAAAg7Yqr+gR0RM32baKQojM\nco33K9hMmL6hPqJmc/Wc/zm15E25EVGzAbzgFI2IiIqiAi7XWv66sCsFDhWZu1KIUfFrw4o1O6Lm\n/lIgtbyKq809evKm+7Z/kJoXEdE3F+mZJWVhBeduq1hfo+ixY0cKw+bTe+mZ172++sYKAABgkMEK\nAABgkMEKAABgkMEKAABgkMEKAABg0FatgC2S29wKmjp6RWNVFLUhzS0/s6Jt72g/P3O1ys88PsnP\nLDqfStraeLTsZstecF4cHOZnVrQhVVwTFddDxRpTJLNt94GKxq6S9kYu11q0ZfL7PeU/c0yL/KbB\nEnP+NdaL2uYqrt2KzIrn7ekw/z7Y1wXNlVfwjRUAAMAggxUAAMAggxUAAMAggxUAAMAggxUAAMCg\nrVoBo7WIve3+T67S1/mtTamthe/TKxqmCo61HRQ09KyLmvGy3bv7uI/gg6togONyvee3zhU0TMX5\nWX5mxXo4FTSklhxnfgPafFbwGUXEdHScH1rRWLZf0BDL5fqcfr5VNK7FXv612yvWwgK94j4QRc+x\nFc3WFc3eFetW0VzwKL6xAgAAGGSwAgAAGGSwAgAAGGSwAgAAGJTXRPH7Mff8zIL9ee/mFgRXvP4C\n8ztvp2e2/fySjVaxsbpiI2VETVEBj1a0yThVxQbbzUV+5uLx3jY+sIKCmKlqE3RFmc2ulKFwuTal\n39Pa8UlqXkREPz1Nz2x7y/TMijKbVlHkExFxUbBu70opRMXnpLwCAABgtxisAAAABhmsAAAABhms\nAAAABhmsAAAABm1f79TzmuzasqBdqqilpc35jSp9s0rPjILjLGnwO8jPLFHRBhkR/WJdksvDeu/R\n17nXWlsWNFBWnGu9oA2x4tyteO2toFms4v2M2J2W0KqWVC7VptzffVc0+FWYV+fpmdNhQfNmRXtf\nREkz3nyWv8a0iobYgrbsPl1vK7BvrAAAAAYZrAAAAAYZrAAAAAYZrAAAAAYZrAAAAAZtX+nRMttK\ndmiuK2gqaXsFrVUFStobK9p0Cpp0Kj73iIhW1DbIw1pr+dfasuDaTV1b76toL5wLGuwqmgYrMota\nZyvWw17R4LcqaLLlkfqc3WaW347WCtbCabEbrcFV7b75n3tNK2LFGlPRuNvP81smr7JDkw0AAMCT\nyWAFAAAwyGAFAAAwyGAFAAAwaLtWgt5zN9kWbODv65rNtSVFExWb1eeCDcstf/7um4LPqeCllxRi\nRESbCkoFuD4F18TOFLrsF2wsP7uXHlmysbqqvKKAgpzd16Yn/3ffvaDQpFc8G/X8IqrpIL8QIiKi\nn+avh/NZfulQxWcfcTc98brXwif/qgUAAHjCGawAAAAGGawAAAAGGawAAAAGGawAAAAGbdcKOE0R\nmS0om/wWrLYsaluraNsrmGv7uuA93c9/T9tBQbNYxWe0LDhOdl9B41z0OT9zLvjdWV/nZz7r9ra7\nFfMM6D36Ovdam46OU/MiItrhUXpmhX52mp9ZcR+IiOnkJD2z4tmwR34rYFvmN3CXtHpfwTdWAAAA\ngwxWAAAAgwxWAAAAgwxWAAAAg7bbMdvniPV53k+fe17WA1PLz4yIaBWbwPM3q1cUTcRikZ85F2zU\nP7mZn1lxnBER52c1uVyuJa8LBcU7JeUrFetW9nsZUVIS09oOlWxMBZ9T0cZ6rkmL9POiVxTknCc+\nE97XCp452s1b6Zlxml+IERExF+RWPBtWlKFUPBf3Vf45ehXfWAEAAAwyWAEAAAwyWAEAAAwyWAEA\nAAwyWAEAAAzarhUwWsTeMu+nr1Z5WQ9UNA1GRMSONCwVFPjFRUG7VuZ59MA7b+dnVrQsRkRfF5z7\nPFp261pF41rFNVFxnIstbxsfRMUaU9Eudb47121JQ2xFyySXam2K6eAwNzQ7L6KmIbVijal43ix6\nPpgqmpgrGiFb/vP2vMo/n0raC6/6edf60wAAAJ5CBisAAIBBBisAAIBBBisAAIBBBisAAIBB21Wv\nTFNuq0xFY1VF5g7pBe1araKhpuBzms/P0jOnqaVnRhS9p1yutYhdeL/ngtamiiaoiswKZQ2xBSra\n9ipe/w5cRk+NinVrfZ6bFxExVTwfFDQNtvx7eS945qjSKtaYgvtqW+a34/Zrvmf5xgoAAGCQwQoA\nAGCQwQoAAGCQwQoAAGDQduUVETUbFRPNq4LNmRHRFtu/VV9LL9igWbLxb12wkbTAdHRUEFp0vs/P\ndsnKtWotYrmfmznnrzN9nb8RumW/7oiI1So/cy9/fY39gtdetAm65HOqYN26Nn2e88sRKsoG0hOj\npsyloLyiHR6nZ5apKC4pWA+fhmIv31gBAAAMMlgBAAAMMlgBAAAMMlgBAAAMMlgBAAAM2q6KqfeI\ni3XeTy9obWqZx/f+3Io2nYr2k4LM9GaiiGiHBQ1+vednHhzmZ0ZErAua1Xi07HOjYJ0paYaraHGb\nCnrAdqRtrqqxqhesB61i7SponeVybbGIdvNWbmhFo2fFeVbQ7lxxjVXdxyvWmV7w2fe5oBVwyv++\np+I4r+IbKwAAgEEGKwAAgEEGKwAAgEEGKwAAgEEGKwAAgEHbtwI+4W1mrdXMin2zI61VbbuP9ANl\nFjT49fP81p/oBQ01O/K58zVMyS1LBY2mJc4rWgELmvEqWgELGuz6nYJ1q8o1N2FRIPt5pqLBr+C+\nGwXNcO1GcsNiRE0TcUREwXNHRUtoyfN2y2+dbVWf0yP4xgoAAGCQwQoAAGCQwQoAAGCQwQoAAGDQ\ndk0HrUXsJW4Irtj0uCjYWB0RLfJz+8U6PbNkE3jBBsVWsfm/YMNnVWlJW+aXjPAI8xxx953czILN\n1SUy1+sHKtatikKMVX7RUjs4SM+MiOgVpVCLgjWm4p7No2Xff9YF5Ss7UmAQp3fzM+eaUoR5tSMl\nORcX6ZHtqKAsreBecJUdeToAAAB4chmsAAAABhmsAAAABhmsAAAABhmsAAAABm1XG9R7RGZbyX5B\nw1JFC1ZESdtgQe9NjYqGnoI2mZjyj7MtCtoLI0oaDLlCr2lvytTv5bdWtZOb6ZmxrFi3d6Qls2Ld\niqJ7wa40wHG53iM2yefbhz6cmxcRUdFoWdE+WXHuVrQwR8RU0bxa8WxccM+K45P0yDZfb5upVRIA\nAGCQwQoAAGCQwQoAAGCQwQoAAGCQwQoAAGDQ461iqmhGq2hTichv54mI2C9onKtovqloftkVRe19\nXSvg9cputiz4/NrBYXpmSbtWhYp2rSm/ybXkPhBR8/oPj/Mz2W1f+Z3HfQQfyJzZPn1fm/KfjXpR\n29x0VHDtnp7mZ1a0uZ7ey8+8Zr6xAgAAGGSwAgAAGGSwAgAAGGSwAgAAGLTdzrPWIvYP8n76Wf4m\ntapSgLYo2Ag9F2z86wXlHRUbFHvPz8wuKIiI2Cv63cP5WU0uD2stYpF8Dq/zN1dHRXnFapWfubfM\nz6y4HiqKfKrKQOaC9bCgAKBk3eZyFetWRUlKxblb8RyX/V5GzRITEdEL1sN2cjM9s+I+WPEM3yru\nWVfwjRUAAMAggxUAAMAggxUAAMAggxUAAMAggxUAAMCg7WpSen/i28xK2vsiaupfNhf5mRWtTRXv\naUXrz1TwGV0UtCxGRKuqE+Jhfc5vL6poGSpo8Ktol6q4JirW7V7wfrZlQUNqRMRU0GhacS8oat3l\nEr3nt1BOBdfZpuA6O8xvSC15Nix4P8uUPBsntoTf1/YKmlcrGnev4OkOAABgkMEKAABgkMEKAABg\nkMEKAABgkMEKAABg0FYVR73PqU1LJQ1LVW1rraC1aS5oWKp4/SXvacFrL2rwK7G//7iP4BnS8tub\nduWaKGiCKmm0LMgsa/CrMBc0+BWcT32X1tgd1zebmN96MzVzOjpOzYuI6OsdOScKGi3ns5qW7F5w\nrFPFM0dBE3MrOM759a+mZ17FN1YAAACDDFYAAACDDFYAAACDDFYAAACDttrd29qUu7Gsz3lZD1SV\nAiSWdjxQsUGx5e9VrymFODjMz6w4zrngHOV6tRax2IEig4KCnFZQXlFS3FGwFlYUd5RkRhS9pxfp\nkSXnE5dqy2VML39dbujbb+XmRUT0/OKV6egoPTOzeO2B6bDgOSZqng0r7i/TjZvpmRVr4XV/g+Qb\nKwAAgEEGKwAAgEEGKwAAgEEGKwAAgEEGKwAAgEHbVWW1FrG3zPvpBa1FMec31ERESdtgy3wvH5jy\nm1/6vbvpma2iFbDCXNDOExFxUXDuc7ne8xsjCxqWKs61nWkerVDRrFWl4j3dkfOJy82np3H68z+f\nmrl84bnUvIiaBr/59DQ9M7XR+oGi66GkFbHgWOc776Rntin/+552mP9+XsU3VgAAAIMMVgAAAIMM\nVgAAAIMMVgAAAIMMVgAAAIO2awWMyG2d6/lzXT+7l54ZEdEWBbVNU0FmwXta0uCX3dIWETHP+Zmt\n5ncPvRe8fh6hR/Tkc6OiGq+gJbQtD9IzS67dXWkarFJxf1kWNNme5jfEcrm2XMbBRz+SmjndzG8F\nrFgPKtrm+mqVnlnyXBgRfZ3fGtyW2z/uPw6tYN2Ko+P8zCv4xgoAAGCQwQoAAGCQwQoAAGCQwQoA\nAGDQ493NVlAMULLxrUrv+ZH38jcXt8Oj9My+LthIWlQ0UWGXjnXX9bnHfHaWmjld82bY37f9gvVw\nfZ6feVjwflaUbFTZbPIzz3PP+YiaAgAu15bLmF7+aG5owfNRX+SX7sTH8o9zuldQiFFUXlFilb9u\nJ1bZ/f8KjnP+ypfTM6/i6Q4AAGCQwQoAAGCQwQoAAGCQwQoAAGCQwQoAAGDQ9q2Ac2KT3V5BKWGf\n8zMjShoMS4614Dj7eUGbzLLgs98raCeqaOuqzOUhbZpiOj7JDT04zM2LKGlxi7lgjZkKmrAKmqB2\nSUlLasF6WJHJ5frZWaw//7nUzOVHk1sGI2rWrXVBo+dLH0mPbOdF9/Gj5PtVRMQi/5mrV6wxLb9r\ncPrw16VnXvnzrvWnAQAAPIUMVgAAAIMMVgAAAIMMVgAAAIMMVgAAAIO2qwnpc8Q6sb2pohVwuZ+f\nGRFxejc/s6Bdq6RtL7MJsjLzoqBJqKIBLSJiym++4XK99/TWtZJPr6C1KeaC1qqK46xoSF0UXLs7\n1ObZV/lNg9at69OWy1h+JLfJbv7w16fmRURMp3fSM9uqoGmw4hnu8Cg/MyLiza/mZxasB20//3m7\nV7RMXvO67RsrAACAQQYrAACAQQYrAACAQQYrAACAQVvuQm65m/krNlZXWR7kZ5ZsBF7mR+5I0UQv\nyGyt6HcPVbk8pEXB57gsuM4qNtjOBedZRenQrhRN7B/mZ0ZE25XXv1dw3nOpPs8x30suXOj59/L5\n6EZ65uaFl9Mzp4uC8obkUqT3co8SS+IeZJ6dpmf217+SnjnfLSgZuWae7gAAAAYZrAAAAAYZrAAA\nAAYZrAAAAAYZrAAAAAZtX++U2Spzmt9SEvv7+ZkR0c/P0jNbRcNSn/MzKxQ0DVY0+KW3Mj3QKhoh\nuUzvc8zJ1+9U0ehZ0TxasR4WtG+WNMSWtJle5GdGRGwKciuOtaJpkEu1vb2YXnwpNbO/+supeRFF\nzzGr/Fa86bCg0fPm8/mZESXtm/PJrfzMD30kPXNaFzQirvOf36/iGysAAIBBBisAAIBBBisAAIBB\nBisAAIBBBisAAIBBW7YC9tTWuV7QLtUq2roiahqmKhqWFov8zIrjLGpvzDZVvJ8Ru9Pe+DRoU7Ts\n862iCaugDakkcyq4Jioa7Cqu3YrPPSJiWZBbkVnQvMojtJZ+vp194bdT8yIi1nfupWf2i/xnjsOX\nPpSeuTh+PT0zImI6Oc4PzWz0vq+dr9Iz54Jno/leQQP5FaySAAAAgwxWAAAAgwxWAAAAgwxWAAAA\ng7Yqr5hX61h96bW0H758MX8zYZW23LLn44M4OMzP3JGSjX6av5lwvns3P3OVvzkzImIu2PTJ5fr5\nKs5+/TdTMw9f+VhqXkTsTDFAK+hE6AUblltB505JkU/UFDn1inXbunVtNnfuxJ2f/lepmctbN1Lz\nIiJu/Tvfmp5ZsRae/uqvpWfe/fyX0zOrnL3xTnrmnS/fSc+cC4pLlkdFpUOPsBt3cgAAgCeYwQoA\nAGCQwQoAAGCQwQoAAGCQwQoAAGDQdlV3c4/N3bw2t/Wbv5GW9cB0sJ+eGRGxd3yUntmW+U0lfZ3f\nLtUq3tOKdq3FIj2yTS09MyJiUXA+cbm23IuDj7yUmnnx+hupeRERMeX/nquiGW79dn4TVMz5rYAV\n9k6KrtuCFrSqeyHXY1ou4zB53fryz/2b1LyIiPbZX0/P3DvMfzZ64du+KT3z6A98PD0zIuL8C19I\nz6xo23vxuZP0zM15/jPs4kArIAAAwE4xWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAzaqhWw7S/j8JWP\npf3wzd27aVkPTIeH6ZkREdPJjfTMfpbXsPheZkELWMVrr2hAi72C5pfVeX5mRPSXPlqSy8Pa3l5M\nL7yYmjkd57chxYdup0f2g+P0zP23X0/PjIK1sKJ5tGJ9LdPzmxZbxXnPpb7UPhr//fITqZn/3n+Z\nf9+92OQ35y4X+efu7eN30jOPp4J1KyJO/t230jN/bf2N6Zmf+1L+evDZX8pvnf38v/7N9Mx3/cil\n/61vrAAAAAYZrAAAAAYZrAAAAAYZrAAAAAa13vsH/5db+92IeLXucIABH++9v/S4D+JJY92CJ5p1\n6xLWLXjiXbp2bTVYAQAA8DB/FRAAAGCQwQoAAGCQwQoAAGDQ3uM+ALbXWnsxIv75/T9+JCI2EfG7\n9//8Hb33VdHP/YcR8T0R8cXe+7dX/Azg6fQY163viYgfjIhFRPyPvff/oeLnAE+nx7F2tdZOIuJf\nRMT+/f/8T733H8j+OeRTXrHjWmufiIg7vfe/83v++xbvfr5z4s/67og4jYi/Z7ACfr+ua91qrS0j\n4nMR8ccj4rWI+FRE/Jne+y9n5APPlmtcu6aIOOq9372/jv2riPjLvfdPZeRTx18FfIq01v5Qa+2z\nrbV/HBG/EBHf0Fp7833/+59rrf39+//8cmvtx1prn2qt/Vxr7bu+Vn7v/Scj4vWyFwA8c4rXre+K\niF/svb/aez+PiB+JiD9V9VqAZ0fl2tV7n3vvd+//cT8ilhHhm5AdYLB6+nxLRPxg7/1bI+KLV/x7\nPxQRf7v3/kcj4s9GxIOL/ztba3+3/jAB3lO1bn0sIn7rfX/+wv3/DiBD2TNXa22/tfaZiPidiPhf\ne+//T+6hU8Eeq6fPr37Ar4r/RER887vfXkdExAuttaPe+89GxM+WHR3Aw6xbwC4qW7vu79369tba\nCxHxv7TW/q3e+y+mHDVlDFZPn7vv++c5Itr7/nz4vn9uUbhhHGALVevWFyPiG97356+Pq3+rDLCN\n8meu3vsbrbX/KyL+ZEQYrJ5w/irgU+z+Jso3WmvfdH8j5J9+3//8zyLi+x78obWmjAJ47JLXrZ+J\niG9trX28tXYQ7/4VnB/PPmaAzLWrtfbh1tpz9//5ON79xuuX8o+abAarp99fi4h/GhE/He/uL3jg\n+yLij7XWfr619tmI+EsRX/Pv+/5oRPzLePdB5QuttT9feuTAsypl3eq9ryPi+yPiJyLisxHxj3rv\nn6s+eOCZlfXM9dGI+MnW2r+OiJ+LiP+t9/5/1B46GdStAwAADPKNFQAAwCCDFQAAwCCDFQAAwCCD\nFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAA\nwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCD\nFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAA\nwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCD\nFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAAwCCDFQAA\nwCCDFQAAwCCDFQAAwCCDFQAAwKC9bf7l2zdP+iu3n8/76S0v6oG+vsgPjYi2KJhB556fuStaxYe/\nQ+/nlP/6P/3qa1/pvb+UHrzjbt886R9/6YXUzJIzreT8LcjckXWrz/PjPoQPrFWshxWsW9cm/Xmr\nyq7cdwuOs2826ZkREW1vq0fzD5Y55T/D9oL7SysYDHrROfrp3/jtS9eurT69V24/Hz/1ie9NO6i2\nyD95Vq/9TnpmRMTeczfTM+ez8/TMqHiYKLgg2zL/sy8Zqosezqbjo/TMG//Z33g1PfQp8PGXXohP\n/o2/mprZe8F5UXCTrrjx9/NVfmbB+1mxvvaLmgepqWA9rGDduj6v3H4+PvkDfyU1s+SXDRc1v8zO\nNhesW5u7d9MzIyKWL34oPXM6Ok7PrLi/tMUiPbNfrNMzIyKO/5O/funa5a8CAgAADDJYAQAADDJY\nAQAADDJYAQAADHq8O2YLGoYWN/I36EVELG7dSs9sy9P0zOkof3NxyQbFlj/Tz6f30jMrijsioqwU\ng0u0yF9rKjoMKjbtnp3lZxYUTVQUz0yHB+mZ8738NTsiou3lf/Ztb5meWdWCxiV6j75O3nRf0Y5W\nsG5VFPlUFE3Mq5rijpKiiYrCpQIla0zFOXoF31gBAAAMMlgBAAAMMlgBAAAMMlgBAAAMMlgBAAAM\n2q4VsPfUtpbNnTtpWQ+0/f30zIiIzdtvp2e2gqaS+TS/tariOHtFSUtF80tFi1JlLg/pmznm7Eao\ngs+vHRzmZy7zm+FKzPnv51qlzeMAACAASURBVFRxL6hqxStYu1pBK2K/W9C8yqXaNKW3w1U0w1U8\nH1x89fX8zNPz9My94/wW5ojd+Zz6ZpWeGQWN0WXr9iP4xgoAAGCQwQoAAGCQwQoAAGCQwQoAAGDQ\n1uUV8ypvs9rmXn7RwvLkJD0zIqKv1/mhU/5c2/YKNqtPLT+zYLO6QgguNbVo+7kb+ftF/npQkRlz\n/ibo+Sx/E/iiYhN4wfo63biRnhkRNWUoBcUl00FNORSXaC1ib7tHtK+lv/1Wal5ERE8+xogoeebY\nf+5meubi+efSMyOKnuMKlJTFTQUlG+f596yr+MYKAABgkMEKAABgkMEKAABgkMEKAABgkMEKAABg\n0HZ1Lq3FlNg01A8P07Les9nkZ0ZEL8rdBRXNL31zkZ4ZLb9JqKJZKyLKzlMu0SOiJ7fjFZxrF199\nPT2z4vytaEidz/OboCqaxaaKe1bUtICVNNkWnPc8So+Yk+8Ti4LrrEDmc+Z7Cq7dqvWgoiG2FXz2\nfV3wHBf5mRWv/Sq+sQIAABhksAIAABhksAIAABhksAIAABhksAIAABi0XStgRGor0OLkOC3rgVbU\n0jIVtIpMBzXHmq6gXaukoaaiaW/u+ZlcrxYRLfd3SMlx75ryQyuuiYprdzo8SM+sUNHeFxHRltvf\nir+WvjpPz6w4R7lcn+eYT09TM2ua4Qoa7AqaiNvhUXpm7NesW63i2r3Ykba9gufN636Os0oCAAAM\nMlgBAAAMMlgBAAAMMlgBAAAMyt8xu4WKoomqzcVRUYxQsUmvYFd9Pz9Lzyz5nCo+oyol7Qdcl4vX\n30jPbInFQA/M6/wNy4ubN9IzK7Rl/hpTslm7SCvYWL9Lr3/n9YiY59zMZ/jz6+tVemYrKISIiOg9\n+XOPopKRRcEIUfAYd93rlqc7AACAQQYrAACAQQYrAACAQQYrAACAQQYrAACAQY+3FbCiqaOoGa6d\n3MwPXZ/nZxY0i7WD/PbGCgUdi2UqWn94hN6jX+Q2IvWCdWZxcpKeOR0XrAf7+/mZU8Hv+CqaN4va\npbLPz4iIviloLKtosuVyreWfb89wG22reO0Fa2FERMtug4yIdnicnhkVzzEFa2HMPT/zCs/uVQYA\nAJDEYAUAADDIYAUAADDIYAUAADDIYAUAADBou1bA1qItl4k/PTHrvn5+lp4ZEdEq2k925PWXtOlU\ntGvtFZRc9po2mVZTXsll5h7zWe510QtahqabN9IzK7SCdatCX60e9yE8VhVNi31d0NjFpdrUYjrM\nbeSdT09T8yJqGlJTnzPvq2ji3amOzIq2vQoFz5u9X+9r940VAADAIIMVAADAIIMVAADAIIMVAADA\noK13+/c5bwNgRSlCSdHCDil5/VPBFs1WkFlRNFGwMZdr1lpMyZuhFy+/lJoXEdGOT9IzS0wFxTMF\nZTZt7zw9MypKjKKmHKmkYOUot0yBa1Zw320F127mc+YDU0XpTlFBTi8omigpnqlYtytKd665yOjZ\nnkIAAAASGKwAAAAGGawAAAAGGawAAAAGGawAAAAGbdUK2KYppqPjvJ9e0CgSR0f5mRERFW17Bc0v\nJe9pURNWuorWn4r2woiIWdvgtek95uRGpMVBfjva/NWvpGdWtGu1/f38zOTWxoioafTcodbZtty6\n9JcnSN/MMd+9mxxa0Jxb0TRYsB70gvWgohExImra9tITa+4FUdBmet3r9u7cJQAAAJ5QBisAAIBB\nBisAAIBBBisAAIBBBisAAIBB29UGtZbbvHZ+lpf1wGFRK2CFikaZima8inatCnsFLVhVjYiT32lc\nm8UiFrdupUa245PUvKrM2FzkZ2Y2wz5Q0F4Y+wf5mRX3rIiI1Xl+5kXBZx87ci94CrTFFNNJ7pow\nn+Wfv21RcN8taIYracksaptrFZfZXv6x7sr5dN0NqZ7uAAAABhmsAAAABhmsAAAABhmsAAAABm21\no6vPc/Sze2k/vC3307LeMxdtrq0ocKgor6goW1jnb6zu6/yN1W2/4HyqKu44OKzJ5WF9jr5aPe6j\n+JpaxXpQsAk8Ltb5mRXXWcW9oOL9jIjoRbnZKgqCuFTvPf0+2QpKk0ruuyVrYcGzUdV1O7WCzPz3\ntBU8b7aiQpDrtPuvAAAA4DEzWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAzaquKnRXJjR0FDTVQ1ilyc\n5WdWtCKeFxxngXZ8kh+6yW8aLGkniqhp/eFSbbEX063nc0MrmrAq2vZ2pWFpr+A4KxpSKz6jiIi9\nZX5mK1hjdqW9kOtT0OhZ0eJa0braK9aYiJr3tCCz5D2NguM8uZmeeZUduesCAAA8uQxWAAAAgwxW\nAAAAgwxWAAAAgwxWAAAAg7ZqBYzWclvSKhqrKpqQImpam85O8zMrWuzm/JaWmsauglbAiva3iJLW\nHx6h9/w2t4pWx3VB49yyYN2aC5rhdqUls2o9qLA4yM+0bl2fHtGTm25bwXowr87TM6cbBS1uBY2e\nba75bqKiGa/k2XhXWmev+f6yI+8KAADAk8tgBQAAMMhgBQAAMMhgBQAAMGi78oqI3A1w6/xNj5G8\n2fM9BcUIvWIzZUXJRoF+fpae2QqKO+Y776RnRkRMh4cluVyi9/xN96tVbl5ETUHOs1yIUbDGxN72\nt8wPpKJ4p6LIqKociof1nn5ezGf5z1ztIL/QpZ/mF3tVPG/FXFDCVaTt55fZZJerRNQUrJTcC67g\nGysAAIBBBisAAIBBBisAAIBBBisAAIBBBisAAIBB21UctRaR2SxS0dJSpPeC9peKFrtVfuvPVNAm\nU6Gvd6ShJiJ6dksdj9Za/rVW8fntSuNaRcNSK/gdX0Ur3i6pOJ+K1kMe1haLmG7eSs2c791NzYvY\nnSbitp/fXlj22guOtaTJ9iJ/jalod654NryKb6wAAAAGGawAAAAGGawAAAAGGawAAAAGGawAAAAG\nbdcKGJHaNNQLGmr6eneaBktaq3pPj6xoGqzQFtufzl8zc1nQzhMR8907Jblcovf8Fr+9/HMtCs7f\nmAqa4eYdOc6SFqyidqmK86ngXlBxz+Zy/eIiNm+8kZo5nZyk5kVEtIP81uDN66+nZ7ZlwTW2rGn3\n7e+8nZ85FzRbF6hoBaxqd34U31gBAAAMMlgBAAAMMlgBAAAMMlgBAAAM2n43X+KG2HZ4lJb1XubN\nW+mZERFRUYpRsVl9XVA0MRdsgs4uE4iIdpR/PqWXHtzX9q53M+UzbZoiDg5zM8/PcvMiSsoGMsuG\nSl0UrDEX+Wt2Pyv43KNoc3XL/71p3xSVd/Cw1tILF0quiYJzYjoueDbcrymiKlFw7U4Vr7+gaKIX\n3Fv76Wl65lV8YwUAADDIYAUAADDIYAUAADDIYAUAADDIYAUAADBou8qZ3iNWea1zfb1Ky3qgXexQ\na9Gc3zg3F7RWTfsH6ZklDT0VDX59zs/kevU5vy1zr6DRs6IldFdUtCwWtJmWtPdFUUtqfmEX12ma\noh0mt5lWXBMH+c8HJWthxb18lf8MG1HTtNgWBa3JBVpB02C7cSM98yq+sQIAABhksAIAABhksAIA\nABhksAIAABhksAIAABi0ZfVKT22yq2j/iKmoCqnnt+lUtNRMxyfpmSUq3s+CZq1+sU7PjIjo65pc\nHtY3c8x376ZmTjdupuZFRPSze+mZFSrO3Tbl/46v5DizW9oe5LaC33FOLT2y5J7NI/T0Fr+SNt4K\nFefZRUErYME1FlH0HFfwLFPRQl3SvFrQhnkV31gBAAAMMlgBAAAMMlgBAAAMMlgBAAAM2rK8okVk\nbrI9KNgIXFAIERE1m982F+mRfb1KzyzZWF2gF5RXpJ7v71ewWZ/Ltb29mG5/ODc0scTngdYKNkIv\ntlziP4jNO/mZBZvVp8Oj9MydUnCOXvcm8GdZa1O0o+RzeJX/fFDiJL8cKJYFxR1np/mZETWlGHP+\nvWBaHqRnVtwL+t2Ce9YVPN0BAAAMMlgBAAAMMlgBAAAMMlgBAAAMMlgBAAAM2q4mpPeIxOa1kqaO\nima4iLp2uGR9dZ6e2U5O0jN7QTvRfHaWnrm4UdBOxPXqPSK5LbOf559rFWtXRVNm21umZ/b1Oj2z\npM20oLEqIiIqGiErGvwq2sq4VJ83Mb/zdm5owXnWlvnrQato2ytoYY6LgsyI6BcF62HFul3Rwv0U\nrFu7MS0AAAA8wQxWAAAAgwxWAAAAgwxWAAAAgwxWAAAAg7ZrBZymiIPDtB/eKhqWClrxIuLdZrH0\nzPxGlbbc7iN9XPqc/9pLGvyq2mS6dq1r01p6q2e79XxqXkSUtAK2gnapzHvAA62iFW9RsBZWtEFG\n1LQC9oJmtYrj5FJtbxnT7Q/nhhbcd2Mq+P386b38zIrjLGoJLXk2rmiErFhjK9obKz77q37ctf40\nAACAp5DBCgAAYJDBCgAAYJDBCgAAYNBWO8/6vIl+727eT68oBijYAB5RU7awK9pymZ45HZ+kZ5Zs\nUCz63FtBAQDXKHMdfKBgc3Ffr9IzKwocWnK5SERE7BVsrL4o2FgdEb2iuKSkyMjvYq9LPz+P9a/+\nSm5owT2yFTzHTSf5zwdtL/85pqosrGI96Kv8e8F0dJSe+TSwSgIAAAwyWAEAAAwyWAEAAAwyWAEA\nAAwyWAEAAAzaqtKkTYtot56rOpYci0VJbOu9ILSgFfHsND9zV1Q0+FU0DXK9Wos4OMjNrGicK1DR\n6FliLlhfK1pnK+4DEdH29/NDK9bDotfPw9pyL5YfeTk39Mat3LyIiIK2vZLzbFPQ6Hn3Tn5mRLRW\n8Porno13Zd2+Zp4aAQAABhmsAAAABhmsAAAABhmsAAAABhmsAAAABm1XbdVabkvaxTov64F5k58Z\nUdN8syhoFjs4zM/cFL2n2YoaIUtUNEJyuXmOuHc3N7Oiba/iOqu4JtYF6/Z+cmtjRM37WXXdtoLf\ncR4V3AusW9emzz3ms7PczLffTs2LiGgFa8x8mt9uPK/zWwFb0fWwuHkjPbMVrLG9YI3tp+fpmdfd\njusbKwAAgEEGKwAAgEEGKwAAgEEGKwAAgEHbtSfMc8TpvbyffnySl1WtYnPxVLCxvKAPY2c2gZ/n\nbvSNiJrPPSJif78ml8tllu5E1JwXrednVhRNVDhLvK88UFE4VLUJuuR8Klhj16v8TC7Xe8RFbuFC\nRdlAy15bI2LvD/zB9MyKsrBeUboTEe0sv7yjpCyuwp38gpWeXALztfjGCgAAYJDBCgAAYJDBCgAA\nYJDBCgAAYJDBCgAAYND2NSk9sbkqufEmImqa4Yr0gpaWtihoGjw4zM88L2iXqnjtVefTpuDc53Kt\n5Z8bFS12FS2hBU1YsT5Pj5wLWpumgmWr37ubHxoRraIltOJ8yrz/c7UW6evW3ksvp+ZFRM3zwWnB\ndVbRNne3Zj3YrPKfj3rBM9fi5o30zOnoOD2z3byVnnkV31gBAAAMMlgBAAAMMlgBAAAMMlgBAAAM\nMlgBAAAM2qoyqs9z9MSWtIqGpen4JD0zIlJfd6W+2eSHFjTUVLSAteUyPbOv85sbuV59nvPXmjm/\nHa1vCprxChqWdsV8epqeWdLeF1HTrLaXvx5Gxf2FS7XFXkwfup2aefFbr6bmRURMJ/nPXNMLL6Zn\nxsnN9MipIDMiYprn/NCK9WC9I+3Od9/Jz7yCb6wAAAAGGawAAAAGGawAAAAGGawAAAAGbVVe0RaL\naDdv5f30O3fysh4o2lzbKjb+TS0/s2BTfcVxLirez4qN5QXFHRER846UoTwN2t5etNsfyQ09zS/e\niXfeTo/sBedvOzhIz6woHepn+eUVZWU2BffCVrEJvCKTS/WLdWx+50upmdNhfklKWaFLtqng3K0o\nb4iIaAXPhquCZ479gtKdintrxWd/1Y+71p8GAADwFDJYAQAADDJYAQAADDJYAQAADDJYAQAADNqq\nFTBbOzpKz5zfejM9MyKiLQta7NqOzLU9v6GmLQuahCoaq05u5GdGxFRw7nOFObkt9KCgXWvvsS7H\nj1fBWtimgsz0xAfBBfeCgubRrs302rTlfiw++vWP+zC+toIm5n4jsX26UElbdERNK2D2PTCipsGv\n4tmw4jivsCNP9gAAAE8ugxUAAMAggxUAAMAggxUAAMAggxUAAMCg7Wqoeo9Yr/N++v5BXtZ908lJ\nemZERF+t8jMvEt/LB5mZn0+hvn4rPbMVtAK2g4KGmtidz+mpMM/5DWkFTVixX3CuTQVNmRUqGquq\nGrsqVJxPBS2TbSrrReT32mwi3k6+T1Y05xZoh8f5oQXPW2Uq1sMKFQ1+5+f5mdfcwO0bKwAAgEEG\nKwAAgEEGKwAAgEEGKwAAgEHb7W6dpoijxE2FFRvfbtzKz4yIaPmbdqeLi/TM+eAoPbMVbKRsp3fT\nM2OVXFBQqN+587gP4dkxLSKOb+Rmnp/m5kVEnN7Lz1zkFxiUKChaKCmEWBdsrI6IuWA9aAeH+ZnH\nNeVQPKxvNrF5++3UzIoypn6eX+w17UhZWMkaU2S6WfBs/MLt/MyK0qGK580r+MYKAABgkMEKAABg\nkMEKAABgkMEKAABgkMEKAABg0FZVTP1iHfNXvpz30wsaVdp+QdNgRLSKppKCJqypoAkqCloBo6D1\nJyo++4rj5HrNm/RWoF7QPtqmRXpmSfPqlP/7uM1RcmtjkXlZsL5GxOJebvtbREQ7K2jCWlsPr83h\nYUzf9C2pke1O/nnWCzIv/vAfSc9sF/nn7sXBbqxbERF75/nNo3vvvJ6eGW9+NT1y8+Zb6ZlX8Y0V\nAADAIIMVAADAIIMVAADAIIMVAADAIIMVAADAoK1q6dpyP6aPvlJ1LDkKGkUioqZxblcUtDdGa/mZ\nc8/P7HN+ZtS1V/II2c2Wr30hNy8iYlHQCnjz+fzMgqbBxWl+Y1XM+dfuYn4jPTMiIs7P8jMvLvIz\ni9ZDLtGm6AfHqZGb4+dS8yIi9g7zn7kqWjIrzt2DezVtc62ifTO5GTciap65lgfpkYvn88/7q/jG\nCgAAYJDBCgAAYJDBCgAAYJDBCgAAYNBW5RV9WsR8dCPth/e9ZVrWA4uKUoSIiIrNhBWZBRuWe8HG\n6lawUb+fn+dnrvIzuV794iLmr34lNbMdHKbmRUTMZ/mbi/ub+Zur57P89aBXFOQUmA7zP/eImvWw\nQltu9cjAgDZvYrqbW+Iw3ckvhTj7/OfTM++9ll+IMa/zn432bx6lZ0ZELA52o9xq7yT/9Zc8G17z\n/cU3VgAAAIMMVgAAAIMMVgAAAIMMVgAAAIMMVgAAAIO2awW8806c/9S/SPvhyxeeS8t6z4u38zMj\nIqaCGbSiCeokr7XxgZKexT7nR56epmfOq4Lmxojo5zW5PKxfXMT69TdSM0uai+aenllxnS1OTvIz\nn7uVnjm98GJ6Zn8uPzMiYt4/yM/cy89sBecTl7t4+5148yf+eWrm8uZxal5ExN5xfjPc3mF+Y/SN\nb/um9MzpRv7zVkREL3juaAUt3Js776RnXryVn7koOEev4hsrAACAQQYrAACAQQYrAACAQQYrAACA\nQQYrAACAQVu1Ap69cTc+908+lfbDN6tNWtYDJy/lt95ERLQpvxtv2stvFts72Ooj/UCOX8xvAdus\nLtIzK97PKiUNcFxqfe8svvRzn0vNfPGbP5aaFxFxcPuF9MzN3fx2qXn1Vnrm6tUvpmduVr+Unrm+\nm988GhGxKvic+pzf4Le+p830uuydHMfz3/FHUjMvviG/GW/v9dfSM9/8J/97euZm9SvpmYv9/Oet\niIh3vvjV9Mw3Xn09PfN3fjo/c3GU/33PC/92fuvsVXxjBQAAMMhgBQAAMMhgBQAAMMhgBQAAMGi7\nnXff+Iej//A/S/vhH1vmb3pc9PxShIiITcvfpDj1/PKOisw7i+fzMy/yCzHeOMsvLvniGwfpmRER\n/+YX7+WH/oP89/Sp8LFvjPlv/XBq5JttnZoXEfHO5kZ65qbnF7rc2nsnP3Odv1l7UbAWrpf5n1FE\nRGv76ZkXkZ95tjlKz4wffzE/8ynw+vLl+NGX/6vUzLdezy9Neu3L+Wvhd37vf5qeebiXvx68fJxf\n5BMR0Vr+53S+yn8+eP0r+evhL37+PD3zN38t//4SERE/c3mJlW+sAAAABhmsAAAABhmsAAAABhms\nAAAABrXeP/gmudba70bEq3WHAwz4eO/9pcd9EE8a6xY80axbl7BuwRPv0rVrq8EKAACAh/mrgAAA\nAIMMVgAAAIMMVgAAAIMMVjuotfZia+0z9//zWmvti+/7837hz/2HrbXfba19pupnAE+nx7hufU9r\n7XOttV9prf3XVT8HeDo9jrWrtfbx1tr/2Vr7bGvtF1pr/3nFzyGf8ood11r7RETc6b3/nd/z37d4\n9/OdE3/Wd0fEaUT8vd77t2flAs+W61q3WmvLiPhcRPzxiHgtIj4VEX+m9/7LGfnAs+Ua166PRsSH\ne++faa3diohPR8R/YO168vnG6inSWvtD93+78Y8j4hci4htaa2++73//c621v3//n19urf1Ya+1T\nrbWfa61919fK773/ZES8XvYCgGdO8br1XRHxi733V3vv5xHxIxHxp6peC/DsqFy7eu+/3Xv/zP1/\nfjsifikiPlb3ashisHr6fEtE/GDv/Vsj4otX/Hs/FBF/u/f+RyPiz0bEg4v/O1trf7f+MAHeU7Vu\nfSwifut9f/5CeDgB8pQ/c7XW/mBEfFtE/N85h0ylvcd9AKT71d77pz7Av/cnIuKb3/32OiIiXmit\nHfXefzYifrbs6AAeZt0CdlHp2nX/rwH+zxHxV3vvd4aPlnIGq6fP3ff98xwR7X1/PnzfP7eI+I7e\n++pajgrg0arWrS9GxDe8789fH1f/VhlgG2XPXPeLMX4sIv5B7/3Hh46Sa+OvAj7F7m+ifKO19k2t\ntSki/vT7/ud/FhHf9+APrTVlFMBjl7xu/UxEfOv9hq2DePev4HhAAdJlrl33yzB+OCI+03v/oYLD\npYjB6un31yLin0bET8e7+wse+L6I+GOttZ9vrX02Iv5SxNV/37e19qMR8S/j3QeVL7TW/nzpkQPP\nqpR1q/e+jojvj4ifiIjPRsQ/6r1/rvrggWdW1jPXd0fEfxgR//77qt3/ZPGxk0DdOgAAwCDfWAEA\nAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwy\nWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEA\nAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwy\nWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEA\nAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwy\nWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAwyWAEAAAza2+Zfvn3zpL/y0vN5P33ueVm7qLX8\nzP6Mv6fZKj6jiIjI/5w+/Rtf+krv/aX04B13++Zxf+XFxHUrIvpmk5r3bmj+OdH2FumZNaqus2S7\nsxzUKDhHP/2br1m3LnH75kl/5XbuulWi4B5Zc9vND+1Vz1t9rsnNVvHyd+i5+NOvXv7MtdVg9cpL\nz8cnf+D70g6qr87TsnbSouChZ0ce+EpUXJAVn1FEyXt68hc+8Wp66FPglRefj5/6638xNXP9xlup\neRFRcjPde/659MwKbdqRvzxRtR5UrNsF62FfX6Rn3vjLf9O6dYlXbj8fP/WJ703NLLnOCq6JVnGd\ntfzX3i/W6ZkREX21KslNNxcMgBWf/UX+uhURcfIX/7tL164duZsBAAA8uQxWAAAAgwxWAAAAgwxW\nAAAAg7Yqr8g23bj5OH/8Vvr5WX5oxabP/YP0zKoNmukKNqeWtfNUbYLnYS3S3+825RcDLG7mF01M\nh4fpmSUqrt0CJZvqo6hlssKulIw8DVpEW+Q+orXlY33k+8AqrofWCkq4ipqtn/Xyjmy9omTjClZJ\nAACAQQYrAACAQQYrAACAQQYrAACAQQYrAACAQY+3Iqagca2vL9Izq/TVKj90fz8/k3y70gL2NOiR\n/n4vTk5S8yIipqPj9My+KVgPe0ET1iZ/LWwH+Y2IVe19FS1gc0WT7TW3az3bWkR2+2hBi11Jg19B\ne+GuHGdE0bNhQZNttPzMVtE8ulzmZ17BN1YAAACDDFYAAACDDFYAAACDDFYAAACDDFYAAACDtqs0\n6T36xTrth2dmvaegpSQioi0K2l/m/Nff1wXvaUmbTkFLS0HLJLuvTVNMx8ktfhUNSwWNXSXHWaC1\n3fgdX1UrYMm9sKLBr6IRkuuzl/8c0woyK1rcWkXTXpF2eJQfWnDttmV+C3VfFzTEFrSuXmU37mYA\nAABPMIMVAADAIIMVAADAIIMVAADAoO12HbYp2n7eZrW2V1BgsEObi0sKHApUbNguKdnYJde8mfKZ\n1lrJpu1sfVOwabegFKIXlMSUlDcUXGNVm6D7Rf572g4P0zOf+XX7WvX8Qpu54PmoonSn4Dj7+iI9\nsy2L7iub/Pe04jmu4vXv0lzwKL6xAgAAGGSwAgAAGGSwAgAAGGSwAgAAGGSwAgAAGLRdpUfvue0a\nBe0fFc0nERF9ld/YVdIM11p6ZNV7mq2isauqubHkfOJSvffo69z3u+Zcy2tcfU9Bu1ZrBddEQXth\nTPlrYUkDWkRNg+FT0K71bGvp53BJM1zFeVahYj2oWLciIqaCltCoeN4seP0Vz8XXfI76xgoAAGCQ\nwQoAAGCQwQoAAGCQwQoAAGCQwQoAAGDQVq2ArbXcBpgdahhq+/mNXRWZFa1Vfcqfv/uc33qzS0o+\ney7VIqJltxeVtMPlKGQ+eAAAGUNJREFUr4clLWDL/Nam3gvWg3XBWri5SM+MiGgla2zF69+de/bu\n6/nrTEUzXsG129cF11lJK2BBZkT0Vf7rr2iyrfjsY7X7a4xvrAAAAAYZrAAAAAYZrAAAAAYZrAAA\nAAZtVV7Ro+duXq3Y+Ja9Sb0wt69W/1979xJrWXqeBfj997nXpS9udTANuRhiLlEGDKLEkAEDAlIk\nQGKCIkVCgggJKTOExBSGRAgETKIIBpFAEHGJATEAjAdBRsQKioJzsRXbka/Bbru7q7uqzm2f/TOo\nKquTPlXdR+v7Tveuep5Jd7VL715777X/tb6zz/+6PHNbjnMcHJZndpxPpWUtb9OyWZ9Lzc0mm+P7\npZmroxuleUnThu0O64aN1Q3r1ubitDyzo2QiaSqFuKhft+d6+zeWb42LTS7uvlUauXPzZmlekmxO\nTsozO4z9g/LMeXJcnpn0lHvN9Xl5Zkd5R0+Rz/Xeb/nGCgAAYCGDFQAAwEIGKwAAgIUMVgAAAAsZ\nrAAAABa6UivgGKuM/f26R29oFMmc9ZnZnga/lma8LWnwy85OfSbbbyQpbhrallbHjiaoedHQCrhz\npUvRe9PQtDe7ri/nHe9T/fPvaG/kMcbIaq+4lbbhGtlwF9dzb7RqONJNz+dhdeOoPrShzbVjjem4\nZo1rvje0SgIAACxksAIAAFjIYAUAALCQwQoAAGAhgxUAAMBCV6pimptNNsf3yx58dVjfNtfRUtKl\no6lkc9bQ2LXX0ajS0E7U0fyyd1CfmWQ0NN9wubHayermrdrQhna4jubR0hbXh1oa7E5P6jM3Dc2N\n5/Xra5KM3Yb1sOG9H9UtdTzeapQ38ra0Ou7Xn7uz4XPWsW6N4rbZ7+i4vnQ02Xa0UO82rDHX3OLr\nGysAAICFDFYAAAALGawAAAAWMlgBAAAsdKXyimSWbqrb3L1blvWdzIYNikmyOjpqCB31mQ2bHrNp\nyOx47g0bc+e9t8ozuV5zc5HNvdq1pqUopWNjecdnt0PD6zl2r3h5ey/WPeUVLTo2wDddX7nETOZF\n7fk2z05L85K0fHbTUELWUYiRhtKZpKd4Z1vKkbKuP850XAuewDdWAAAACxmsAAAAFjJYAQAALGSw\nAgAAWMhgBQAAsNCVqjLGWGV1cFj24HNd3zC0e/u58swkyaxvacmqp1Gm2ji8UR/a8N6n8Nx8pKG7\n8IGONh0eYySj9p3saG1aHTacEw1rzFg3rFsNLaEtLWA7Pe1S1e1vSTL29uozO5orudQYI6v9g9LM\n2XEf09Hg19BoufPC8+WZLU2uScv95mxYDzq09Ng2nKNPYpUEAABYyGAFAACwkMEKAABgIYMVAADA\nQgYrAACAha5UcTTnLG3yGx3NaB2tN0myqe8qmeen5Zktz7+hwW+enZVnthzneUN7YZJxetKSyyXm\nTNbFrWs79c14m5OGc6KhXSsNjYhZ1f+Mr+P17GrFG7v159OsPueTbNbX265FrdGwbrU04zUc5+xo\nhts0NI+mpyW0oxlvFLdWJum5ZnVkPoFvrAAAABYyWAEAACxksAIAAFjIYAUAALDQ1cor1uucf/u1\nsgfff+WVsqxHLu68UZ6ZJKujG+WZlUUgj4zdvfLMluPcu9Kp9/7p2PDKtRo7q6xu3aoN3WvYtNux\nYbmhwKBjY3nGqI9sWAu3SccG+JbyAy4158zmrLbgauw1fCYair06zrOW8opV/bqVJKudhutLw7F2\nvKYt733D9eVJfGMFAACwkMEKAABgIYMVAADAQgYrAACAhQxWAAAAC12pmm2MkVVhm9v61W+WZT1y\ncVzbovPIlnTYtTSqjNHQJNTRXnh6Up45Dg7LM5Mkc9OTyzvN1DdXndevM/N8O1rcOlpCO5rFWhq7\nulpCR8PPODsauzpa5bjUWK3K24g7Prsd7ZMdetatnuv47LiPW9WvMZv7x+WZHY3R171u+cYKAABg\nIYMVAADAQgYrAACAhQxWAAAACxmsAAAAFrpa/cYYpe1Fp996rSzrkf3nb5dnJj2NKhn1rVXzvKH5\npqOx6ri+TaajFXDunJVnJj1tbTzGGMlucdNQw3owtqUZrqEpczasB9tknvW02VbbnNSvsTzGnPWf\n34b2zZbGtY7jbGgi3qZ2347nv6otrUySzIamxdnV5voYvrECAABYyGAFAACwkMEKAABgIYMVAADA\nQlfe0V256X6s6ssbNufr8swkWa0bNoHvbkmBwZZs0Bz7++WZXZseWzbScqm5ucjm3t33+zDeXUOZ\nTWb9JvAc3y+PnB3rdsP1pWONeZB70JJbbWzJteCpMFJ+Do+dhvO3oYipo4iqwzxv+jw0rNsdpRAt\n15eO47xmvrECAABYyGAFAACwkMEKAABgIYMVAADAQgYrAACAha7UCjh2d7Pz4otlD350+1ZZ1iPj\n8EZ5ZpKWhqlsGhpV1uf1mR0Ndh3tUmdn5ZFd7X1dbYNcYoz697GjcW7U/5xrc3ZantnSYLd6tn/G\nNy/qWxFnx3rY1IrIZUayKm7c29Rfd+Zp/T1Hx1rYsmbvbdHnoaPBr+E+rqMhtrLN/L14tq9mAAAA\nBQxWAAAACxmsAAAAFjJYAQAALGSwAgAAWOhKrYCZM6lsM+toRutq/+ho22tobWppLzyvbxbraETs\naNqbpyflmUky11oBr8sYI2Pvakvdu9praMZraFhadbS4NayF46Dh9ezQ0KqW9KxdY6f4nE/qP0c8\n1pwz87z2HmEcHJbmJcmobi5MelpCO+43O5r2ksyO+80GHa3JHQ1+1/16+sYKAABgIYMVAADAQgYr\nAACAhQxWAAAAC11pJ+rMrN1k21U00aFj42OHhs2ELTo2E67rN/+Pjs3/STK2Y3Pq02Ek1RusL9a1\neV0aCgxa1piWjeX160FHyUSSjNHwM869hiKjhtIhLjdG/Ub+eXxcmvcgtP5zlobPQ0vxStPnYezV\n33dUF6E8CN2SNfaaZw3fWAEAACxksAIAAFjIYAUAALCQwQoAAGAhgxUAAMBCV6pJGWOUNqvM8/pm\nrc0br5VnJmlprRr7B/WZHW17Dc03m+P75ZlZ1f+cYHXjZnlmksxzrYDXZW42mSfFbVgdLW4Nxv6W\ntLg1rK+zo62sqQVsrhqOdUuKbHmcUb7OjIP6e47M+s/EPGtosGvQssYkGev6e+OW5tEG1U2YSd/7\n9Djb8UoDAAB8gBmsAAAAFjJYAQAALGSwAgAAWMhgBQAAsNDVKv7GKjm8Ufbg42Z9+8dY1WcmSTZb\nUrE0Rn3meX1Dz2p/vzwzHY2IXRqalLjcWK0yqtsdd/dq85KkoQ2po21vW46zo12qbY3puG41NItx\nzarP4Y71oEFLe+FOXaP1I2O3qW2uoxWxYz3suN88OiyP7GhZfBLfWAEAACxksAIAAFjIYAUAALCQ\nwQoAAGChq+3mm5vk5H7do+81bFC8aNqktiUbgWfD5urZseF107Dpc+XnBLzT3Gwy79+rDR3159ps\nWLvGlnwmZsN6MBo2q7dZ1W8Cn+f114Ktek15p2e5dKfj3rDr87At5V47DefTyXF9Zsc5+gTbcdUF\nAAD4ADNYAQAALGSwAgAAWMhgBQAAsJDBCgAAYKErVpqM2jasjuaT04a2uSQ5OqrP3MzyyNHQLjVW\nDY0qHW06HY2IZ2flmUkyOs4nLjV2dzNefOn9Pox3NTrOtdmwHjY0LLWsMR2NiB1tpkkyG64F5YlJ\nRksql5kzqV4Tdp/hVseOe46G+60kyV7D/cE8rM/sWA8PtmjdfgzfWAEAACxksAIAAFjIYAUAALCQ\nwQoAAGAhgxUAAMBCV6tJGSPZ3at79OP7dVkPzfOmFrfKNsSH5sVFeWaLVf1xjk195jxf12eenpRn\nJsnm+Hpbap5pm4vk3lu1mQ3rQRrWg9nQlNnSClh5XWnU8nqm5/qyOTstz+xoL+RyM7P8HuHizhul\neUmyamh37rg36sjseO5JMvYPWnKrbRru4Vvasg+v9/X0jRUAAMBCBisAAICFDFYAAAALGawAAAAW\nulp5RbWDw/LItk1/HRu2O8or5rNbijB2G0o2bt0qz0ySnPdsgued5qzfuDwOGsoWOtbDpvKVcg3r\na0cZSEfJRNKzsX6sGgox7h+XZ3K5sbOb8cKLpZm7t58rzUuSnDQUGDSshTnrKTZr0fL868tsdg4a\n7rd3GsaSptKhx/GNFQAAwEIGKwAAgIUMVgAAAAsZrAAAABYyWAEAACx0tfqNzSapbJmqLynpaVNJ\nkvv36jM7mrC2pRVw1fDcO9pkmlrAuD5jjIyudaFSR0vobkN74ZY0+GU2NEHt79dnJhkdr2mDncOj\n9/sQnhlzvc7m26+WZq46WpNXoz6zozV3NBznel2fmfTcb3a8T5tZn9nwms5rvi921wgAALCQwQoA\nAGAhgxUAAMBCBisAAICFDFYAAAALXa1GbbVKbt6qe/RVw1y339T+Vfm8Hzk/q8/saL7p0PHcz+oz\nN2/dKc9M0tcmxDvMzSabu2+VZo6OBsqO1qYOHU1QDeZF/Wdstd/z3DfH98szZ0fT4pa890+Dsbef\n1SvfUxt6elybl7Rcd7emMbnpOr6pbN9+aDQ0xM6G42x576+Zb6wAAAAWMlgBAAAsZLACAABYyGAF\nAACw0NV2YM+ZnBRufmzYTNeykTJJ9vfrMzuKJkbDrNzxPh3eqM88vlceuWoqA5kd5R1cbjSUTTQU\nTcyThg3Lew0bljcNm8AbzJPT8sxNRyFE8uDaWmx1VL/GzrP615TLzfVZ5je+Xpo5OooBtqVoouM4\nb9ysz0wyGp5/R3nF6Hj+He/TNZe6+cYKAABgIYMVAADAQgYrAACAhQxWAAAACxmsAAAAFrpSVdac\nm8zK1r3T+haslla8JKPhWDdb0rC02j94vw/hPZkNTTrz/Lw8M2loqeOxxhgZe9WtgPXNRR3n2jg4\nLM9sWbcbjJv1jVVz3bMetLRWNTRXcn3Gaifj1q3a0I5Wy47MbWk3vl/fRNxlNqzb8+5b5ZlPQ5Ot\nb6wAAAAWMlgBAAAsZLACAABYyGAFAACwkMEKAABgoStVZY2d3YwXP1T36KcNrXhdTUgNzTero6Py\nzGxmfebFujxynhe2Sz7S0U40G17PJHNLGiGfDqO+EaqhHW51dKM8s8M4bFi3Oqzqf244LvbLM5Om\ntsGOa1ZD0yJPUN2Od9TQEtpx7m7L/UHTWjjmdrxPo6O9saHdOcfH9ZlP4BsrAACAhQxWAAAACxms\nAAAAFjJYAQAALHSl8oqM4k3g27JBsUtH0cTBQX1mR89CQ3HJ2K/fWN5UhZLsNbxPPMas37i72qnN\n2yYdhS6j4ZO2adgE3XGcadoEvtuQ2XGcXGqen2f9ta+WZo69q93yvRc7zz1XntnirOF+c7f+9UyS\n7DfcH1QXOCU999sdJRsN5/2TWCUBAAAWMlgBAAAsZLACAABYyGAFAACwkMEKAABgoatVZcxZ2gIy\n33qzLOuR0dF8kiQ7DS1gs779pOU4z+sb/OZZR9Vgg1VXC1jDe8/lVjvJ7RdKI+fhUWlem1V9G9K4\nqD93Z0fbXkOD3ehosk1a1thnvnV3y43d3ey+/HJp5mw4Jy7erL+Pmw3n2aqjNfjwsDwzaWojPmg4\n1obXtOUedq/hOJ/AN1YAAAALGawAAAAWMlgBAAAsZLACAABYyGAFAACw0NVbAU+Oyx583H6uLOs7\nulqLOppKOsxNfWZHu1ZDm848OanPPF+XZyY9rUc8xpzJurbJbtxveP86movW9+oz9w7KI0dLm2n9\netB2fVk3rDMtTVhNrYi8085O8lxtm+m4W9/gt9PRxNzwOeu45s7zpnbfzazPPK1vHh179a2zm4b7\nuLFTf5xP4hsrAACAhQxWAAAACxmsAAAAFjJYAQAALHSlHV1zvc7m9W+XPfj4yEfLsr6Ted60uXbV\nsbm64Vg7Mg+PyiNHGjIPb5Rn5rRhA3ySTVMu7zQ3F5lv3inNHAf1BQ65X180Mc+UDZTqKAdKU5nN\nGPWZXJ+Li+TNN2ozV/U/S5/FxUBJTynEar+hdKehvCFJclBf7tVSZjPrSzZWt2sLW5KUl1e9G99Y\nAQAALGSwAgAAWMhgBQAAsJDBCgAAYCGDFQAAwEJXqjQZOztZ3X6u7MHnaGioOWhohksyOxqW9uub\nX1bHd8szW9qlOtoLVw3HWXi+v93q+RdbcnmnixvP584P/Xhp5mpT3+K2uz4uzxwNLXYd63bHcXbY\nPatvbkyScbEuz9w5fqs8czSc91xu7u5lvvxKaeY4vV+alySjYz242dDCvNOQ2XAPl6TlWOfOXn1m\nQ9Ngx732WNevr0/iGysAAICFDFYAAAALGawAAAAWMlgBAAAsZLACAABY6EqtgBkj2a1rFhl3vl2W\n9R2nJ/WZSeZJfe7qxs3yzMxZHrm5W98uNS/q26XmeX3zy+hoEkoyDurbdLjc7sndvPDZ/1kbelzf\n4DfX5+WZ6WjsavhMbI7r28o2Z/XNo23rwarhZ5wHDY1lBwf1mVxqzJlxflob2tHGe3hUn9nREtpw\nz5E7r9Vndml47zdv3CnPnOf118GO+80n8Y0VAADAQgYrAACAhQxWAAAACxmsAAAAFrpSecU8O8v5\nV75c9uB73/eRsqxHNq/UZybJWNdv/JsdG8vPGko2bt4qz8xp8abcJPNefcnGxZtvlmcmyezYnMul\nTl+7ky/96/9Smvn897xcmpckq/2rdQm9F2OM8szZUJAz1/Wbi8eq/rlfnNUX5HTmVtu/1VCIwaXm\nGNns177eFy9+uDQvSbKp/+yu1vX3B6uTe+WZo7DM7fe4W3/f0VHAloY1tuP6ko5yoCc93LU+GgAA\nwFPIYAUAALCQwQoAAGAhgxUAAMBCBisAAICFrlRDdfza3Xz2336q7MFf+v7PlWU9stPQrJUkd79R\n39Jyere+aXBnr35Wnpv6lpabL98uzzz6UH174cHzN8szk6bmGy612t3JzZefK808+q4XS/OSZOzV\nN0ydfuv18sxtafDrWLc6MpPk6KXa8zNJ9p6rXw9Xh1oBr8vdz/9ufukv/f3SzKM/dFCalyS3X2lo\nDW7QcW/U5fz4vDxz76j++nJxXt9uvGm4vsyL673f2p4zDQAA4APKYAUAALCQwQoAAGAhgxUAAMBC\nBisAAICFrlSh9/pLfyy/8JOfLHvwn/yx+2VZj/zhsy+UZybJ5vCl8szDUT/Xns/98szXzp4vz/zM\nq/WtgJ/7Yn3L4hc/+83yzCT58m90nKc/35C5/e6/9Efy6b/2C6WZP3r7V0vzkuRs96g88/6q/nN2\nMXfKM189faE884vfvFGe+bnPn5RnJsmXv/Ct8sx737hXnvna1zvWw3/QkLn9Dn7go/mjH//PpZm3\nz18rzUuSt/Y+VJ55Z13fkrlarcszzy7q77eS5GxT32799fv1jZ5febX+WvDF36lft77yhVfLM5Mk\nn/rIpf/ZN1YAAAALGawAAAAWMlgBAAAsZLACAABYaMw53/tfHuPVJF/qOxxgge+dc778fh/EB411\nCz7QrFuXsG7BB96la9eVBisAAADeya8CAgAALGSwAgAAWMhgBQAAsFD9/70z7cYYLyX5Hw//+OEk\nF0ke/V9L//Cc86zpcf92kp96+MefnXP+s47HAZ4+7+O69XeS/PUkM8mvJfkbc87TjscCnj7v49r1\n1SSvP3y80znnj3Q8DrWUV2y5McbfS3J3zvkPf99/H3nw/m6KHudPJfn5JB9Lsk7y3/LgBuV3KvKB\nZ8c1rlvfmwc3RD+Y5DTJv0vyi3POf1mRDzxbrmvtepj51SQ/OOd8oyqTfn4V8Ckyxvj+McZvjjH+\nVZLfSPLdY4w33va//8QY458//Pc/MMb4D2OMXxljfHqM8bF3if+TSf73nPN4znme5JeS/JWu5wI8\nG5rXrSTZS3KYB7+hcSPJ1xueBvCMuYa1iy1ksHr6/Ikk/3jO+QNJvvaEv/dPk/zMnPOHkvzVJI8+\n/D8yxvjZS/7+Z5L82THGh8YYN5P8eJLvrj104BnVsm7NOb+U5J8k+UqS303yzTnnJ6sPHnhmdd1z\nJQ9+ffmTY4z/M8b4qcf8HT5g7LF6+nxhzvkr7+Hv/ViSP/7g2+skyYtjjKM55y8n+eXf/5fnnL8+\nxvhHST6R5G6SX82D3/sFWKpl3Xq4N+IvJvlIkjeT/Psxxk/MOf9N0XEDz7aWteuhj805vzbG+HCS\n/z7G+K055/8qOGYaGayePvfe9u+bJONtfz5827+PXHHT5Zzz55L8XJKMMX4myecXHCfAI13r1l9I\n8ttzzm8lyRjjF5P8mSQGK6BC5z3X1x7+8/+NMf5jkh9OYrD6gPOrgE+xh5soXx9jfHSMscrv3RP1\niSQ//egPD8spnmiM8V0P//l9Sf5y3JwAxYrXrS8n+dNjjKOHm8v/XJLfqj5mgMq1a4xxa4xx6+G/\n30zy55P8ev1RU81g9fT7u0n+ax78lOOrb/vvP53kR8cY/3eM8ZtJ/mbyrr/v+/GHf/fjSf7WnPPN\nxuMGnl0l69ac81NJ/lMe/OryZ/Kg0fRfNB878Oyquuf6g0k+Ncb4tSSfzoM200/0HjoV1K0DAAAs\n5BsrAACAhQxWAAAACxmsAAAAFjJYAQAALGSwAgAAWMhgBQAAsJDBCgAAYKH/D7B914rjndw+AAAA\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x1080 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeYRJMQaL4Yu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
        "\n",
        "#from ann_visualizer.visualize import ann_viz\n",
        "\n",
        "def get_cnn_model(input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, kernel_size=(4, 4), activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(48, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(120, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYeP29mLL4ZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_all():\n",
        "    filenames, mfccs, durations, sampling_rates, labels, cls_true = get_data('free-spoken-digit-dataset-master/free-spoken-digit-dataset-master/recordings', augment = False)\n",
        "    \n",
        "    filenames_a, mfccs_a, durations_a, sampling_rates_a, labels_a, cls_true_a = get_data('free-spoken-digit-dataset-master/free-spoken-digit-dataset-master/recordings', augment = True)\n",
        "\n",
        "    mfccs = np.append(mfccs, mfccs_a, axis=0)\n",
        "    labels = np.append(labels, labels_a, axis =0)\n",
        "    \n",
        "    dim_1 = mfccs.shape[1]\n",
        "    dim_2 = mfccs.shape[2]\n",
        "    channels = 1\n",
        "    classes = 10\n",
        "    \n",
        "    print(\"sampling rate (max) = \", np.max(sampling_rates))\n",
        "    print(\"sampling rate (min) = \", np.min(sampling_rates))\n",
        "    print(\"duration (max) = \", np.max(durations))\n",
        "    print(\"duration (avg) = \", np.average(durations))\n",
        "    print(\"duration (min) = \", np.min(durations))\n",
        "    print(\"mffc matrix = \", mfccs.shape)\n",
        "\n",
        "    X = mfccs\n",
        "    X = X.reshape((mfccs.shape[0], dim_1, dim_2, channels))\n",
        "    y = labels\n",
        "\n",
        "    input_shape = (dim_1, dim_2, channels)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "    model = get_cnn_model(input_shape, classes)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbjDN3-TL4ZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test, cnn_model = get_all()\n",
        "\n",
        "print(cnn_model.summary())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsrbfB_LVegc",
        "colab_type": "code",
        "outputId": "6564b4bd-243e-411c-8803-a4b584b22861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "cnn_model.fit(X_train, y_train, batch_size=64, epochs=500, verbose=1, validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2160 samples, validate on 240 samples\n",
            "Epoch 1/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.1103 - acc: 0.9727 - val_loss: 0.3494 - val_acc: 0.8792\n",
            "Epoch 2/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0940 - acc: 0.9787 - val_loss: 0.2387 - val_acc: 0.9083\n",
            "Epoch 3/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0699 - acc: 0.9838 - val_loss: 0.2274 - val_acc: 0.9292\n",
            "Epoch 4/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0682 - acc: 0.9819 - val_loss: 0.2784 - val_acc: 0.9083\n",
            "Epoch 5/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0652 - acc: 0.9838 - val_loss: 0.3074 - val_acc: 0.8958\n",
            "Epoch 6/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0534 - acc: 0.9861 - val_loss: 0.2019 - val_acc: 0.9333\n",
            "Epoch 7/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0543 - acc: 0.9866 - val_loss: 0.2791 - val_acc: 0.9208\n",
            "Epoch 8/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0378 - acc: 0.9912 - val_loss: 0.2083 - val_acc: 0.9167\n",
            "Epoch 9/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0459 - acc: 0.9898 - val_loss: 0.3033 - val_acc: 0.9125\n",
            "Epoch 10/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0293 - acc: 0.9935 - val_loss: 0.3244 - val_acc: 0.9083\n",
            "Epoch 11/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0278 - acc: 0.9935 - val_loss: 0.2042 - val_acc: 0.9333\n",
            "Epoch 12/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0192 - acc: 0.9963 - val_loss: 0.1457 - val_acc: 0.9458\n",
            "Epoch 13/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0264 - acc: 0.9912 - val_loss: 0.2148 - val_acc: 0.9333\n",
            "Epoch 14/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0209 - acc: 0.9963 - val_loss: 0.1988 - val_acc: 0.9375\n",
            "Epoch 15/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0178 - acc: 0.9958 - val_loss: 0.2320 - val_acc: 0.9292\n",
            "Epoch 16/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0148 - acc: 0.9972 - val_loss: 0.2702 - val_acc: 0.9417\n",
            "Epoch 17/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0169 - acc: 0.9963 - val_loss: 0.2414 - val_acc: 0.9250\n",
            "Epoch 18/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0238 - acc: 0.9944 - val_loss: 0.2096 - val_acc: 0.9333\n",
            "Epoch 19/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0191 - acc: 0.9944 - val_loss: 0.2120 - val_acc: 0.9292\n",
            "Epoch 20/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0209 - acc: 0.9935 - val_loss: 0.2735 - val_acc: 0.9208\n",
            "Epoch 21/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0125 - acc: 0.9972 - val_loss: 0.2013 - val_acc: 0.9375\n",
            "Epoch 22/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0176 - acc: 0.9940 - val_loss: 0.2739 - val_acc: 0.9333\n",
            "Epoch 23/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0155 - acc: 0.9958 - val_loss: 0.2024 - val_acc: 0.9417\n",
            "Epoch 24/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0091 - acc: 0.9995 - val_loss: 0.1948 - val_acc: 0.9417\n",
            "Epoch 25/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0133 - acc: 0.9968 - val_loss: 0.2181 - val_acc: 0.9333\n",
            "Epoch 26/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0104 - acc: 0.9977 - val_loss: 0.2086 - val_acc: 0.9333\n",
            "Epoch 27/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0198 - acc: 0.9944 - val_loss: 0.1789 - val_acc: 0.9417\n",
            "Epoch 28/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0120 - acc: 0.9972 - val_loss: 0.1875 - val_acc: 0.9250\n",
            "Epoch 29/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0136 - acc: 0.9958 - val_loss: 0.2521 - val_acc: 0.9250\n",
            "Epoch 30/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0107 - acc: 0.9972 - val_loss: 0.2241 - val_acc: 0.9375\n",
            "Epoch 31/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0123 - acc: 0.9968 - val_loss: 0.2186 - val_acc: 0.9375\n",
            "Epoch 32/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0087 - acc: 0.9981 - val_loss: 0.1384 - val_acc: 0.9500\n",
            "Epoch 33/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0071 - acc: 0.9981 - val_loss: 0.2436 - val_acc: 0.9250\n",
            "Epoch 34/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0057 - acc: 0.9981 - val_loss: 0.1804 - val_acc: 0.9333\n",
            "Epoch 35/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0088 - acc: 0.9981 - val_loss: 0.2432 - val_acc: 0.9375\n",
            "Epoch 36/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0096 - acc: 0.9977 - val_loss: 0.2077 - val_acc: 0.9333\n",
            "Epoch 37/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0043 - acc: 0.9991 - val_loss: 0.2206 - val_acc: 0.9417\n",
            "Epoch 38/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0065 - acc: 0.9995 - val_loss: 0.2152 - val_acc: 0.9500\n",
            "Epoch 39/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0082 - acc: 0.9981 - val_loss: 0.2238 - val_acc: 0.9250\n",
            "Epoch 40/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0131 - acc: 0.9963 - val_loss: 0.1842 - val_acc: 0.9375\n",
            "Epoch 41/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0070 - acc: 0.9986 - val_loss: 0.2199 - val_acc: 0.9375\n",
            "Epoch 42/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0113 - acc: 0.9977 - val_loss: 0.2360 - val_acc: 0.9458\n",
            "Epoch 43/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0108 - acc: 0.9972 - val_loss: 0.2070 - val_acc: 0.9542\n",
            "Epoch 44/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0082 - acc: 0.9972 - val_loss: 0.2856 - val_acc: 0.9250\n",
            "Epoch 45/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0075 - acc: 0.9981 - val_loss: 0.2247 - val_acc: 0.9333\n",
            "Epoch 46/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0063 - acc: 0.9991 - val_loss: 0.2166 - val_acc: 0.9417\n",
            "Epoch 47/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0032 - acc: 0.9991 - val_loss: 0.2847 - val_acc: 0.9333\n",
            "Epoch 48/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.2312 - val_acc: 0.9417\n",
            "Epoch 49/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0053 - acc: 0.9986 - val_loss: 0.2684 - val_acc: 0.9333\n",
            "Epoch 50/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0042 - acc: 0.9986 - val_loss: 0.2640 - val_acc: 0.9292\n",
            "Epoch 51/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0046 - acc: 0.9991 - val_loss: 0.2171 - val_acc: 0.9417\n",
            "Epoch 52/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0028 - acc: 0.9995 - val_loss: 0.1928 - val_acc: 0.9417\n",
            "Epoch 53/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.2044 - val_acc: 0.9458\n",
            "Epoch 54/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.2402 - val_acc: 0.9333\n",
            "Epoch 55/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0039 - acc: 0.9991 - val_loss: 0.2455 - val_acc: 0.9458\n",
            "Epoch 56/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0030 - acc: 0.9991 - val_loss: 0.2360 - val_acc: 0.9375\n",
            "Epoch 57/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.1497 - val_acc: 0.9500\n",
            "Epoch 58/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.2244 - val_acc: 0.9500\n",
            "Epoch 59/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0037 - acc: 0.9991 - val_loss: 0.2562 - val_acc: 0.9375\n",
            "Epoch 60/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.2563 - val_acc: 0.9417\n",
            "Epoch 61/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0031 - acc: 0.9995 - val_loss: 0.2367 - val_acc: 0.9375\n",
            "Epoch 62/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.2079 - val_acc: 0.9417\n",
            "Epoch 63/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0060 - acc: 0.9986 - val_loss: 0.2470 - val_acc: 0.9167\n",
            "Epoch 64/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0038 - acc: 0.9991 - val_loss: 0.2085 - val_acc: 0.9500\n",
            "Epoch 65/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0038 - acc: 0.9986 - val_loss: 0.2387 - val_acc: 0.9458\n",
            "Epoch 66/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0046 - acc: 0.9995 - val_loss: 0.3003 - val_acc: 0.9458\n",
            "Epoch 67/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.3067 - val_acc: 0.9458\n",
            "Epoch 68/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0061 - acc: 0.9981 - val_loss: 0.3572 - val_acc: 0.9250\n",
            "Epoch 69/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0066 - acc: 0.9977 - val_loss: 0.3808 - val_acc: 0.9375\n",
            "Epoch 70/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.2428 - val_acc: 0.9417\n",
            "Epoch 71/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0046 - acc: 0.9986 - val_loss: 0.2808 - val_acc: 0.9333\n",
            "Epoch 72/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.2697 - val_acc: 0.9375\n",
            "Epoch 73/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0045 - acc: 0.9986 - val_loss: 0.3334 - val_acc: 0.9250\n",
            "Epoch 74/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.3262 - val_acc: 0.9167\n",
            "Epoch 75/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0044 - acc: 0.9991 - val_loss: 0.2870 - val_acc: 0.9417\n",
            "Epoch 76/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0031 - acc: 0.9991 - val_loss: 0.2594 - val_acc: 0.9458\n",
            "Epoch 77/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0043 - acc: 0.9991 - val_loss: 0.2989 - val_acc: 0.9292\n",
            "Epoch 78/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.2895 - val_acc: 0.9417\n",
            "Epoch 79/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0037 - acc: 0.9991 - val_loss: 0.2390 - val_acc: 0.9375\n",
            "Epoch 80/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.2956 - val_acc: 0.9250\n",
            "Epoch 81/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0020 - acc: 0.9995 - val_loss: 0.2388 - val_acc: 0.9375\n",
            "Epoch 82/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.7058e-04 - acc: 1.0000 - val_loss: 0.2207 - val_acc: 0.9417\n",
            "Epoch 83/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0046 - acc: 0.9986 - val_loss: 0.2872 - val_acc: 0.9458\n",
            "Epoch 84/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0034 - acc: 0.9991 - val_loss: 0.3419 - val_acc: 0.9333\n",
            "Epoch 85/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0014 - acc: 0.9995 - val_loss: 0.3579 - val_acc: 0.9417\n",
            "Epoch 86/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0066 - acc: 0.9991 - val_loss: 0.3751 - val_acc: 0.9125\n",
            "Epoch 87/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.3343 - val_acc: 0.9333\n",
            "Epoch 88/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.3205 - val_acc: 0.9333\n",
            "Epoch 89/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.3189 - val_acc: 0.9458\n",
            "Epoch 90/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0013 - acc: 0.9995 - val_loss: 0.3421 - val_acc: 0.9375\n",
            "Epoch 91/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.2811 - val_acc: 0.9375\n",
            "Epoch 92/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 8.0124e-04 - acc: 1.0000 - val_loss: 0.2345 - val_acc: 0.9542\n",
            "Epoch 93/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0022 - acc: 0.9995 - val_loss: 0.2374 - val_acc: 0.9500\n",
            "Epoch 94/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0083 - acc: 0.9972 - val_loss: 0.3625 - val_acc: 0.9250\n",
            "Epoch 95/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0114 - acc: 0.9963 - val_loss: 0.2384 - val_acc: 0.9458\n",
            "Epoch 96/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0022 - acc: 0.9991 - val_loss: 0.2209 - val_acc: 0.9500\n",
            "Epoch 97/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0021 - acc: 0.9991 - val_loss: 0.2056 - val_acc: 0.9542\n",
            "Epoch 98/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0014 - acc: 0.9995 - val_loss: 0.2147 - val_acc: 0.9500\n",
            "Epoch 99/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 9.3739e-04 - acc: 1.0000 - val_loss: 0.2225 - val_acc: 0.9500\n",
            "Epoch 100/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.2185 - val_acc: 0.9500\n",
            "Epoch 101/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0014 - acc: 0.9991 - val_loss: 0.2580 - val_acc: 0.9417\n",
            "Epoch 102/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.2400 - val_acc: 0.9417\n",
            "Epoch 103/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.5026e-04 - acc: 1.0000 - val_loss: 0.2593 - val_acc: 0.9375\n",
            "Epoch 104/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.2894 - val_acc: 0.9458\n",
            "Epoch 105/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.2965 - val_acc: 0.9333\n",
            "Epoch 106/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0086 - acc: 0.9977 - val_loss: 0.3078 - val_acc: 0.9375\n",
            "Epoch 107/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0125 - acc: 0.9963 - val_loss: 0.2397 - val_acc: 0.9333\n",
            "Epoch 108/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0041 - acc: 0.9991 - val_loss: 0.2216 - val_acc: 0.9333\n",
            "Epoch 109/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.2422 - val_acc: 0.9333\n",
            "Epoch 110/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.2129 - val_acc: 0.9375\n",
            "Epoch 111/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.2687 - val_acc: 0.9458\n",
            "Epoch 112/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0038 - acc: 0.9986 - val_loss: 0.3191 - val_acc: 0.9208\n",
            "Epoch 113/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0020 - acc: 0.9995 - val_loss: 0.2792 - val_acc: 0.9208\n",
            "Epoch 114/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0030 - acc: 0.9991 - val_loss: 0.2331 - val_acc: 0.9417\n",
            "Epoch 115/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0048 - acc: 0.9981 - val_loss: 0.2245 - val_acc: 0.9417\n",
            "Epoch 116/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 9.0654e-04 - acc: 1.0000 - val_loss: 0.2826 - val_acc: 0.9458\n",
            "Epoch 117/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.2856 - val_acc: 0.9417\n",
            "Epoch 118/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0023 - acc: 0.9991 - val_loss: 0.2130 - val_acc: 0.9500\n",
            "Epoch 119/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.2861 - val_acc: 0.9458\n",
            "Epoch 120/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0022 - acc: 0.9995 - val_loss: 0.2497 - val_acc: 0.9542\n",
            "Epoch 121/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0031 - acc: 0.9991 - val_loss: 0.1824 - val_acc: 0.9583\n",
            "Epoch 122/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 9.4165e-04 - acc: 1.0000 - val_loss: 0.2237 - val_acc: 0.9583\n",
            "Epoch 123/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0033 - acc: 0.9986 - val_loss: 0.2687 - val_acc: 0.9417\n",
            "Epoch 124/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 0.2094 - val_acc: 0.9542\n",
            "Epoch 125/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0021 - acc: 0.9991 - val_loss: 0.2571 - val_acc: 0.9500\n",
            "Epoch 126/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.2073 - val_acc: 0.9625\n",
            "Epoch 127/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0031 - acc: 0.9981 - val_loss: 0.2182 - val_acc: 0.9542\n",
            "Epoch 128/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.2249 - val_acc: 0.9500\n",
            "Epoch 129/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.3552e-04 - acc: 1.0000 - val_loss: 0.2303 - val_acc: 0.9542\n",
            "Epoch 130/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.2745 - val_acc: 0.9458\n",
            "Epoch 131/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0021 - acc: 0.9991 - val_loss: 0.2581 - val_acc: 0.9458\n",
            "Epoch 132/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0022 - acc: 0.9995 - val_loss: 0.3130 - val_acc: 0.9375\n",
            "Epoch 133/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0042 - acc: 0.9991 - val_loss: 0.2901 - val_acc: 0.9417\n",
            "Epoch 134/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0064 - acc: 0.9986 - val_loss: 0.2361 - val_acc: 0.9583\n",
            "Epoch 135/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0014 - acc: 0.9995 - val_loss: 0.2057 - val_acc: 0.9417\n",
            "Epoch 136/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0018 - acc: 0.9991 - val_loss: 0.1595 - val_acc: 0.9625\n",
            "Epoch 137/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0018 - acc: 0.9991 - val_loss: 0.2318 - val_acc: 0.9458\n",
            "Epoch 138/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0044 - acc: 0.9986 - val_loss: 0.1988 - val_acc: 0.9542\n",
            "Epoch 139/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.2929e-04 - acc: 1.0000 - val_loss: 0.1639 - val_acc: 0.9667\n",
            "Epoch 140/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.0781e-04 - acc: 1.0000 - val_loss: 0.1749 - val_acc: 0.9625\n",
            "Epoch 141/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.2528e-04 - acc: 1.0000 - val_loss: 0.1628 - val_acc: 0.9708\n",
            "Epoch 142/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.2363 - val_acc: 0.9542\n",
            "Epoch 143/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.1572 - val_acc: 0.9583\n",
            "Epoch 144/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.7712e-04 - acc: 1.0000 - val_loss: 0.1897 - val_acc: 0.9667\n",
            "Epoch 145/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0029 - acc: 0.9986 - val_loss: 0.1864 - val_acc: 0.9583\n",
            "Epoch 146/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 8.7482e-04 - acc: 1.0000 - val_loss: 0.2081 - val_acc: 0.9542\n",
            "Epoch 147/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.2069 - val_acc: 0.9667\n",
            "Epoch 148/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0018 - acc: 0.9991 - val_loss: 0.1893 - val_acc: 0.9667\n",
            "Epoch 149/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0069 - acc: 0.9981 - val_loss: 0.1897 - val_acc: 0.9583\n",
            "Epoch 150/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.2233 - val_acc: 0.9500\n",
            "Epoch 151/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.3250e-04 - acc: 1.0000 - val_loss: 0.2450 - val_acc: 0.9458\n",
            "Epoch 152/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0061 - acc: 0.9986 - val_loss: 0.1765 - val_acc: 0.9542\n",
            "Epoch 153/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.1691 - val_acc: 0.9625\n",
            "Epoch 154/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0030 - acc: 0.9991 - val_loss: 0.1915 - val_acc: 0.9625\n",
            "Epoch 155/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.2035 - val_acc: 0.9625\n",
            "Epoch 156/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.0802e-04 - acc: 1.0000 - val_loss: 0.1909 - val_acc: 0.9500\n",
            "Epoch 157/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.9763e-04 - acc: 1.0000 - val_loss: 0.1791 - val_acc: 0.9542\n",
            "Epoch 158/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0023 - acc: 0.9995 - val_loss: 0.1368 - val_acc: 0.9708\n",
            "Epoch 159/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.1723 - val_acc: 0.9625\n",
            "Epoch 160/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 8.2764e-04 - acc: 1.0000 - val_loss: 0.1864 - val_acc: 0.9583\n",
            "Epoch 161/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.2039 - val_acc: 0.9583\n",
            "Epoch 162/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 8.1577e-04 - acc: 1.0000 - val_loss: 0.1941 - val_acc: 0.9625\n",
            "Epoch 163/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.3803e-04 - acc: 1.0000 - val_loss: 0.2037 - val_acc: 0.9625\n",
            "Epoch 164/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.5340e-04 - acc: 1.0000 - val_loss: 0.2472 - val_acc: 0.9583\n",
            "Epoch 165/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0014 - acc: 0.9991 - val_loss: 0.2822 - val_acc: 0.9500\n",
            "Epoch 166/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 8.8091e-04 - acc: 1.0000 - val_loss: 0.3182 - val_acc: 0.9500\n",
            "Epoch 167/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.3352e-04 - acc: 1.0000 - val_loss: 0.3272 - val_acc: 0.9375\n",
            "Epoch 168/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.0121e-04 - acc: 1.0000 - val_loss: 0.2755 - val_acc: 0.9542\n",
            "Epoch 169/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.4606e-04 - acc: 1.0000 - val_loss: 0.2787 - val_acc: 0.9542\n",
            "Epoch 170/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.1056e-04 - acc: 1.0000 - val_loss: 0.2558 - val_acc: 0.9583\n",
            "Epoch 171/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.0188e-04 - acc: 1.0000 - val_loss: 0.2530 - val_acc: 0.9500\n",
            "Epoch 172/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.1029e-04 - acc: 1.0000 - val_loss: 0.2881 - val_acc: 0.9458\n",
            "Epoch 173/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.1171e-04 - acc: 1.0000 - val_loss: 0.2601 - val_acc: 0.9583\n",
            "Epoch 174/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0021 - acc: 0.9991 - val_loss: 0.2023 - val_acc: 0.9583\n",
            "Epoch 175/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.7300e-04 - acc: 1.0000 - val_loss: 0.2334 - val_acc: 0.9583\n",
            "Epoch 176/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.0136e-04 - acc: 1.0000 - val_loss: 0.2938 - val_acc: 0.9458\n",
            "Epoch 177/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.1477e-04 - acc: 1.0000 - val_loss: 0.2751 - val_acc: 0.9500\n",
            "Epoch 178/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.8747e-04 - acc: 1.0000 - val_loss: 0.2575 - val_acc: 0.9542\n",
            "Epoch 179/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.5241e-04 - acc: 1.0000 - val_loss: 0.2323 - val_acc: 0.9625\n",
            "Epoch 180/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.6764e-04 - acc: 1.0000 - val_loss: 0.2380 - val_acc: 0.9625\n",
            "Epoch 181/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.4943e-04 - acc: 1.0000 - val_loss: 0.2449 - val_acc: 0.9583\n",
            "Epoch 182/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.5423e-04 - acc: 1.0000 - val_loss: 0.3114 - val_acc: 0.9625\n",
            "Epoch 183/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.8396e-04 - acc: 1.0000 - val_loss: 0.3120 - val_acc: 0.9583\n",
            "Epoch 184/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.3060 - val_acc: 0.9417\n",
            "Epoch 185/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.6335e-04 - acc: 1.0000 - val_loss: 0.2991 - val_acc: 0.9458\n",
            "Epoch 186/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0012 - acc: 0.9991 - val_loss: 0.2909 - val_acc: 0.9458\n",
            "Epoch 187/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.9644e-04 - acc: 1.0000 - val_loss: 0.2741 - val_acc: 0.9500\n",
            "Epoch 188/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.2694 - val_acc: 0.9542\n",
            "Epoch 189/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0024 - acc: 0.9991 - val_loss: 0.2851 - val_acc: 0.9625\n",
            "Epoch 190/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.7522e-04 - acc: 1.0000 - val_loss: 0.2634 - val_acc: 0.9667\n",
            "Epoch 191/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.1524e-04 - acc: 1.0000 - val_loss: 0.2625 - val_acc: 0.9583\n",
            "Epoch 192/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.6600e-04 - acc: 0.9995 - val_loss: 0.2933 - val_acc: 0.9458\n",
            "Epoch 193/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0044 - acc: 0.9986 - val_loss: 0.3524 - val_acc: 0.9417\n",
            "Epoch 194/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.2301e-04 - acc: 1.0000 - val_loss: 0.3326 - val_acc: 0.9417\n",
            "Epoch 195/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.7393e-04 - acc: 1.0000 - val_loss: 0.2599 - val_acc: 0.9500\n",
            "Epoch 196/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.2481 - val_acc: 0.9500\n",
            "Epoch 197/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.2278 - val_acc: 0.9542\n",
            "Epoch 198/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.3559e-04 - acc: 1.0000 - val_loss: 0.1838 - val_acc: 0.9625\n",
            "Epoch 199/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.7832e-04 - acc: 1.0000 - val_loss: 0.2304 - val_acc: 0.9458\n",
            "Epoch 200/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.6266e-04 - acc: 1.0000 - val_loss: 0.2538 - val_acc: 0.9458\n",
            "Epoch 201/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0014 - acc: 0.9991 - val_loss: 0.2309 - val_acc: 0.9500\n",
            "Epoch 202/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.1831 - val_acc: 0.9667\n",
            "Epoch 203/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.2462e-04 - acc: 1.0000 - val_loss: 0.1795 - val_acc: 0.9667\n",
            "Epoch 204/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.6082e-04 - acc: 1.0000 - val_loss: 0.2265 - val_acc: 0.9583\n",
            "Epoch 205/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.6615e-04 - acc: 1.0000 - val_loss: 0.2091 - val_acc: 0.9583\n",
            "Epoch 206/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.4509e-04 - acc: 1.0000 - val_loss: 0.2283 - val_acc: 0.9583\n",
            "Epoch 207/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 8.6736e-04 - acc: 0.9995 - val_loss: 0.2748 - val_acc: 0.9500\n",
            "Epoch 208/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.2093 - val_acc: 0.9500\n",
            "Epoch 209/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.7944e-04 - acc: 1.0000 - val_loss: 0.2415 - val_acc: 0.9500\n",
            "Epoch 210/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.6716e-04 - acc: 1.0000 - val_loss: 0.2510 - val_acc: 0.9458\n",
            "Epoch 211/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.8362e-04 - acc: 1.0000 - val_loss: 0.2546 - val_acc: 0.9500\n",
            "Epoch 212/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.7521e-04 - acc: 1.0000 - val_loss: 0.2227 - val_acc: 0.9583\n",
            "Epoch 213/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.0305e-04 - acc: 1.0000 - val_loss: 0.2117 - val_acc: 0.9542\n",
            "Epoch 214/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.9137e-04 - acc: 1.0000 - val_loss: 0.2456 - val_acc: 0.9583\n",
            "Epoch 215/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.0530e-04 - acc: 1.0000 - val_loss: 0.2890 - val_acc: 0.9458\n",
            "Epoch 216/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.9787e-04 - acc: 1.0000 - val_loss: 0.2723 - val_acc: 0.9458\n",
            "Epoch 217/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.4748e-04 - acc: 1.0000 - val_loss: 0.2407 - val_acc: 0.9500\n",
            "Epoch 218/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.9921e-04 - acc: 1.0000 - val_loss: 0.2708 - val_acc: 0.9458\n",
            "Epoch 219/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0015 - acc: 0.9991 - val_loss: 0.3234 - val_acc: 0.9417\n",
            "Epoch 220/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.0132e-04 - acc: 0.9995 - val_loss: 0.2701 - val_acc: 0.9542\n",
            "Epoch 221/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.2287e-04 - acc: 1.0000 - val_loss: 0.1842 - val_acc: 0.9667\n",
            "Epoch 222/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.0844e-04 - acc: 1.0000 - val_loss: 0.1515 - val_acc: 0.9625\n",
            "Epoch 223/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0013 - acc: 0.9991 - val_loss: 0.2499 - val_acc: 0.9458\n",
            "Epoch 224/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.0901e-04 - acc: 0.9995 - val_loss: 0.2715 - val_acc: 0.9458\n",
            "Epoch 225/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.7307e-04 - acc: 1.0000 - val_loss: 0.2421 - val_acc: 0.9500\n",
            "Epoch 226/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.6486e-04 - acc: 1.0000 - val_loss: 0.2274 - val_acc: 0.9583\n",
            "Epoch 227/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.4826e-04 - acc: 1.0000 - val_loss: 0.2215 - val_acc: 0.9542\n",
            "Epoch 228/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.2968e-04 - acc: 1.0000 - val_loss: 0.2398 - val_acc: 0.9542\n",
            "Epoch 229/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.7060e-04 - acc: 1.0000 - val_loss: 0.2442 - val_acc: 0.9583\n",
            "Epoch 230/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.5923e-04 - acc: 1.0000 - val_loss: 0.2191 - val_acc: 0.9583\n",
            "Epoch 231/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.1863e-04 - acc: 1.0000 - val_loss: 0.2477 - val_acc: 0.9583\n",
            "Epoch 232/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.3488e-04 - acc: 1.0000 - val_loss: 0.2413 - val_acc: 0.9583\n",
            "Epoch 233/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 8.1140e-04 - acc: 1.0000 - val_loss: 0.2160 - val_acc: 0.9625\n",
            "Epoch 234/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.9931e-04 - acc: 1.0000 - val_loss: 0.2073 - val_acc: 0.9625\n",
            "Epoch 235/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.2752e-04 - acc: 1.0000 - val_loss: 0.1911 - val_acc: 0.9625\n",
            "Epoch 236/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.5053e-04 - acc: 0.9995 - val_loss: 0.2347 - val_acc: 0.9625\n",
            "Epoch 237/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.2650e-04 - acc: 1.0000 - val_loss: 0.2480 - val_acc: 0.9583\n",
            "Epoch 238/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.0485e-04 - acc: 1.0000 - val_loss: 0.2469 - val_acc: 0.9583\n",
            "Epoch 239/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.3335e-04 - acc: 1.0000 - val_loss: 0.2591 - val_acc: 0.9583\n",
            "Epoch 240/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.1234e-04 - acc: 1.0000 - val_loss: 0.2665 - val_acc: 0.9542\n",
            "Epoch 241/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.8969e-04 - acc: 0.9995 - val_loss: 0.2961 - val_acc: 0.9542\n",
            "Epoch 242/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 8.6016e-04 - acc: 1.0000 - val_loss: 0.2450 - val_acc: 0.9542\n",
            "Epoch 243/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.9831e-04 - acc: 1.0000 - val_loss: 0.2698 - val_acc: 0.9458\n",
            "Epoch 244/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.1323e-04 - acc: 1.0000 - val_loss: 0.2476 - val_acc: 0.9542\n",
            "Epoch 245/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.5264e-04 - acc: 1.0000 - val_loss: 0.2699 - val_acc: 0.9500\n",
            "Epoch 246/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 8.7788e-04 - acc: 0.9995 - val_loss: 0.2105 - val_acc: 0.9500\n",
            "Epoch 247/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.1972e-04 - acc: 1.0000 - val_loss: 0.2788 - val_acc: 0.9292\n",
            "Epoch 248/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.7348e-04 - acc: 0.9995 - val_loss: 0.2418 - val_acc: 0.9583\n",
            "Epoch 249/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.8163e-04 - acc: 1.0000 - val_loss: 0.2819 - val_acc: 0.9458\n",
            "Epoch 250/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.3154e-04 - acc: 1.0000 - val_loss: 0.2685 - val_acc: 0.9458\n",
            "Epoch 251/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.6336e-04 - acc: 1.0000 - val_loss: 0.2042 - val_acc: 0.9542\n",
            "Epoch 252/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.3597e-04 - acc: 1.0000 - val_loss: 0.1643 - val_acc: 0.9625\n",
            "Epoch 253/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0012 - acc: 0.9995 - val_loss: 0.3077 - val_acc: 0.9458\n",
            "Epoch 254/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.3020e-04 - acc: 1.0000 - val_loss: 0.2618 - val_acc: 0.9417\n",
            "Epoch 255/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.3187e-04 - acc: 1.0000 - val_loss: 0.2254 - val_acc: 0.9375\n",
            "Epoch 256/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.9056e-04 - acc: 1.0000 - val_loss: 0.2052 - val_acc: 0.9583\n",
            "Epoch 257/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.7102e-04 - acc: 1.0000 - val_loss: 0.2110 - val_acc: 0.9500\n",
            "Epoch 258/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.4854e-04 - acc: 1.0000 - val_loss: 0.2348 - val_acc: 0.9458\n",
            "Epoch 259/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.5664e-04 - acc: 0.9995 - val_loss: 0.2000 - val_acc: 0.9625\n",
            "Epoch 260/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.4553e-04 - acc: 1.0000 - val_loss: 0.1909 - val_acc: 0.9667\n",
            "Epoch 261/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.8186e-04 - acc: 1.0000 - val_loss: 0.2154 - val_acc: 0.9625\n",
            "Epoch 262/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 9.8119e-05 - acc: 1.0000 - val_loss: 0.2129 - val_acc: 0.9625\n",
            "Epoch 263/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.4929e-04 - acc: 1.0000 - val_loss: 0.2139 - val_acc: 0.9625\n",
            "Epoch 264/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.3476e-04 - acc: 1.0000 - val_loss: 0.2079 - val_acc: 0.9708\n",
            "Epoch 265/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 9.6529e-05 - acc: 1.0000 - val_loss: 0.2101 - val_acc: 0.9625\n",
            "Epoch 266/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.1840e-05 - acc: 1.0000 - val_loss: 0.2050 - val_acc: 0.9625\n",
            "Epoch 267/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.6176e-04 - acc: 1.0000 - val_loss: 0.2057 - val_acc: 0.9583\n",
            "Epoch 268/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.4252e-04 - acc: 1.0000 - val_loss: 0.1756 - val_acc: 0.9583\n",
            "Epoch 269/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.2587e-04 - acc: 1.0000 - val_loss: 0.1927 - val_acc: 0.9542\n",
            "Epoch 270/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.5954e-04 - acc: 0.9995 - val_loss: 0.2117 - val_acc: 0.9583\n",
            "Epoch 271/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.7376e-04 - acc: 1.0000 - val_loss: 0.1953 - val_acc: 0.9667\n",
            "Epoch 272/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.3513e-04 - acc: 1.0000 - val_loss: 0.2314 - val_acc: 0.9500\n",
            "Epoch 273/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.1382e-04 - acc: 1.0000 - val_loss: 0.2052 - val_acc: 0.9542\n",
            "Epoch 274/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.7194e-04 - acc: 1.0000 - val_loss: 0.2167 - val_acc: 0.9542\n",
            "Epoch 275/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.5676e-04 - acc: 1.0000 - val_loss: 0.2150 - val_acc: 0.9583\n",
            "Epoch 276/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 8.1971e-04 - acc: 0.9995 - val_loss: 0.1821 - val_acc: 0.9625\n",
            "Epoch 277/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.1096e-04 - acc: 1.0000 - val_loss: 0.1833 - val_acc: 0.9667\n",
            "Epoch 278/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 9.5379e-05 - acc: 1.0000 - val_loss: 0.1873 - val_acc: 0.9625\n",
            "Epoch 279/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 8.5732e-05 - acc: 1.0000 - val_loss: 0.1792 - val_acc: 0.9625\n",
            "Epoch 280/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.1544e-04 - acc: 1.0000 - val_loss: 0.2105 - val_acc: 0.9583\n",
            "Epoch 281/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 9.0400e-04 - acc: 0.9995 - val_loss: 0.2278 - val_acc: 0.9500\n",
            "Epoch 282/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.8569e-05 - acc: 1.0000 - val_loss: 0.2301 - val_acc: 0.9542\n",
            "Epoch 283/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0015 - acc: 0.9995 - val_loss: 0.1939 - val_acc: 0.9667\n",
            "Epoch 284/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.2292e-04 - acc: 1.0000 - val_loss: 0.1833 - val_acc: 0.9542\n",
            "Epoch 285/500\n",
            "2160/2160 [==============================] - 7s 3ms/step - loss: 2.7799e-04 - acc: 1.0000 - val_loss: 0.1738 - val_acc: 0.9667\n",
            "Epoch 286/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.4608e-05 - acc: 1.0000 - val_loss: 0.1849 - val_acc: 0.9542\n",
            "Epoch 287/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.1013e-04 - acc: 1.0000 - val_loss: 0.2250 - val_acc: 0.9500\n",
            "Epoch 288/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 9.6877e-05 - acc: 1.0000 - val_loss: 0.2180 - val_acc: 0.9458\n",
            "Epoch 289/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.6659e-05 - acc: 1.0000 - val_loss: 0.2254 - val_acc: 0.9542\n",
            "Epoch 290/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.7286e-04 - acc: 1.0000 - val_loss: 0.2544 - val_acc: 0.9500\n",
            "Epoch 291/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.2271e-04 - acc: 1.0000 - val_loss: 0.2176 - val_acc: 0.9583\n",
            "Epoch 292/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.0804e-04 - acc: 1.0000 - val_loss: 0.1868 - val_acc: 0.9625\n",
            "Epoch 293/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.4707e-04 - acc: 1.0000 - val_loss: 0.1863 - val_acc: 0.9583\n",
            "Epoch 294/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0013 - acc: 0.9991 - val_loss: 0.2239 - val_acc: 0.9542\n",
            "Epoch 295/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.0765e-05 - acc: 1.0000 - val_loss: 0.2105 - val_acc: 0.9542\n",
            "Epoch 296/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.7045e-05 - acc: 1.0000 - val_loss: 0.2010 - val_acc: 0.9583\n",
            "Epoch 297/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.9512e-04 - acc: 1.0000 - val_loss: 0.2298 - val_acc: 0.9500\n",
            "Epoch 298/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.3985e-04 - acc: 1.0000 - val_loss: 0.2306 - val_acc: 0.9625\n",
            "Epoch 299/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.0285e-04 - acc: 1.0000 - val_loss: 0.2167 - val_acc: 0.9542\n",
            "Epoch 300/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.9658e-04 - acc: 1.0000 - val_loss: 0.2036 - val_acc: 0.9542\n",
            "Epoch 301/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.6839e-04 - acc: 1.0000 - val_loss: 0.2138 - val_acc: 0.9583\n",
            "Epoch 302/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.4827e-04 - acc: 1.0000 - val_loss: 0.2005 - val_acc: 0.9583\n",
            "Epoch 303/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.8799e-05 - acc: 1.0000 - val_loss: 0.2010 - val_acc: 0.9542\n",
            "Epoch 304/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.9505e-05 - acc: 1.0000 - val_loss: 0.2115 - val_acc: 0.9500\n",
            "Epoch 305/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.1152e-04 - acc: 1.0000 - val_loss: 0.1713 - val_acc: 0.9542\n",
            "Epoch 306/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.4282e-05 - acc: 1.0000 - val_loss: 0.1782 - val_acc: 0.9583\n",
            "Epoch 307/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.7061e-05 - acc: 1.0000 - val_loss: 0.1820 - val_acc: 0.9542\n",
            "Epoch 308/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 8.2155e-05 - acc: 1.0000 - val_loss: 0.1913 - val_acc: 0.9500\n",
            "Epoch 309/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.2142e-05 - acc: 1.0000 - val_loss: 0.2060 - val_acc: 0.9500\n",
            "Epoch 310/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0014 - acc: 0.9995 - val_loss: 0.1624 - val_acc: 0.9542\n",
            "Epoch 311/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.1161e-05 - acc: 1.0000 - val_loss: 0.1688 - val_acc: 0.9542\n",
            "Epoch 312/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.7252e-04 - acc: 1.0000 - val_loss: 0.1659 - val_acc: 0.9625\n",
            "Epoch 313/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.1316e-04 - acc: 1.0000 - val_loss: 0.2210 - val_acc: 0.9542\n",
            "Epoch 314/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.9517e-04 - acc: 1.0000 - val_loss: 0.2104 - val_acc: 0.9500\n",
            "Epoch 315/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.2310e-04 - acc: 1.0000 - val_loss: 0.2281 - val_acc: 0.9542\n",
            "Epoch 316/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0024 - acc: 0.9991 - val_loss: 0.2466 - val_acc: 0.9417\n",
            "Epoch 317/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0011 - acc: 0.9995 - val_loss: 0.2744 - val_acc: 0.9500\n",
            "Epoch 318/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.9437e-04 - acc: 1.0000 - val_loss: 0.2773 - val_acc: 0.9500\n",
            "Epoch 319/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.8678e-04 - acc: 1.0000 - val_loss: 0.2471 - val_acc: 0.9542\n",
            "Epoch 320/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 8.1343e-04 - acc: 0.9995 - val_loss: 0.2055 - val_acc: 0.9542\n",
            "Epoch 321/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.6512e-04 - acc: 1.0000 - val_loss: 0.1973 - val_acc: 0.9500\n",
            "Epoch 322/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.5492e-04 - acc: 1.0000 - val_loss: 0.2091 - val_acc: 0.9583\n",
            "Epoch 323/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.1840e-04 - acc: 0.9995 - val_loss: 0.2077 - val_acc: 0.9583\n",
            "Epoch 324/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0014 - acc: 0.9991 - val_loss: 0.2804 - val_acc: 0.9583\n",
            "Epoch 325/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.8661e-04 - acc: 1.0000 - val_loss: 0.2744 - val_acc: 0.9500\n",
            "Epoch 326/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.1875e-04 - acc: 1.0000 - val_loss: 0.3105 - val_acc: 0.9500\n",
            "Epoch 327/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.0361e-04 - acc: 1.0000 - val_loss: 0.2912 - val_acc: 0.9583\n",
            "Epoch 328/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.6305e-04 - acc: 1.0000 - val_loss: 0.2769 - val_acc: 0.9458\n",
            "Epoch 329/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.8717e-04 - acc: 1.0000 - val_loss: 0.2842 - val_acc: 0.9542\n",
            "Epoch 330/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0011 - acc: 0.9995 - val_loss: 0.2336 - val_acc: 0.9500\n",
            "Epoch 331/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0030 - acc: 0.9995 - val_loss: 0.2366 - val_acc: 0.9500\n",
            "Epoch 332/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.7854e-04 - acc: 1.0000 - val_loss: 0.2365 - val_acc: 0.9542\n",
            "Epoch 333/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.5832e-04 - acc: 1.0000 - val_loss: 0.2764 - val_acc: 0.9500\n",
            "Epoch 334/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.8513e-04 - acc: 1.0000 - val_loss: 0.2835 - val_acc: 0.9583\n",
            "Epoch 335/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 9.2695e-05 - acc: 1.0000 - val_loss: 0.2741 - val_acc: 0.9542\n",
            "Epoch 336/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.2957e-04 - acc: 1.0000 - val_loss: 0.2657 - val_acc: 0.9583\n",
            "Epoch 337/500\n",
            "2160/2160 [==============================] - 7s 3ms/step - loss: 6.3697e-05 - acc: 1.0000 - val_loss: 0.2607 - val_acc: 0.9542\n",
            "Epoch 338/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 9.6437e-05 - acc: 1.0000 - val_loss: 0.2818 - val_acc: 0.9542\n",
            "Epoch 339/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.1054e-05 - acc: 1.0000 - val_loss: 0.2860 - val_acc: 0.9542\n",
            "Epoch 340/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.1395e-04 - acc: 0.9995 - val_loss: 0.2701 - val_acc: 0.9542\n",
            "Epoch 341/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.8362e-04 - acc: 1.0000 - val_loss: 0.2696 - val_acc: 0.9583\n",
            "Epoch 342/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.0359e-04 - acc: 1.0000 - val_loss: 0.2802 - val_acc: 0.9542\n",
            "Epoch 343/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.3121e-04 - acc: 0.9995 - val_loss: 0.2657 - val_acc: 0.9583\n",
            "Epoch 344/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.1710e-04 - acc: 1.0000 - val_loss: 0.2281 - val_acc: 0.9583\n",
            "Epoch 345/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.3450e-04 - acc: 1.0000 - val_loss: 0.2604 - val_acc: 0.9583\n",
            "Epoch 346/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.8559e-04 - acc: 1.0000 - val_loss: 0.2443 - val_acc: 0.9625\n",
            "Epoch 347/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.4080e-04 - acc: 0.9995 - val_loss: 0.2326 - val_acc: 0.9542\n",
            "Epoch 348/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.9210e-04 - acc: 0.9995 - val_loss: 0.2878 - val_acc: 0.9542\n",
            "Epoch 349/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.9022e-04 - acc: 1.0000 - val_loss: 0.2711 - val_acc: 0.9583\n",
            "Epoch 350/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.1476e-05 - acc: 1.0000 - val_loss: 0.2685 - val_acc: 0.9583\n",
            "Epoch 351/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.3147e-04 - acc: 0.9995 - val_loss: 0.3564 - val_acc: 0.9458\n",
            "Epoch 352/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.3919e-04 - acc: 1.0000 - val_loss: 0.3393 - val_acc: 0.9417\n",
            "Epoch 353/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.1482e-04 - acc: 1.0000 - val_loss: 0.3125 - val_acc: 0.9458\n",
            "Epoch 354/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.0257e-04 - acc: 1.0000 - val_loss: 0.2919 - val_acc: 0.9500\n",
            "Epoch 355/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.5156e-04 - acc: 1.0000 - val_loss: 0.3008 - val_acc: 0.9583\n",
            "Epoch 356/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.6523e-04 - acc: 1.0000 - val_loss: 0.2987 - val_acc: 0.9500\n",
            "Epoch 357/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.7267e-05 - acc: 1.0000 - val_loss: 0.2997 - val_acc: 0.9542\n",
            "Epoch 358/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.3157 - val_acc: 0.9458\n",
            "Epoch 359/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.6403e-04 - acc: 1.0000 - val_loss: 0.3041 - val_acc: 0.9542\n",
            "Epoch 360/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.6083e-05 - acc: 1.0000 - val_loss: 0.3223 - val_acc: 0.9500\n",
            "Epoch 361/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 8.6585e-05 - acc: 1.0000 - val_loss: 0.3210 - val_acc: 0.9500\n",
            "Epoch 362/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.4450e-05 - acc: 1.0000 - val_loss: 0.3106 - val_acc: 0.9500\n",
            "Epoch 363/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.8204e-05 - acc: 1.0000 - val_loss: 0.2918 - val_acc: 0.9542\n",
            "Epoch 364/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 9.2049e-05 - acc: 1.0000 - val_loss: 0.2830 - val_acc: 0.9583\n",
            "Epoch 365/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.9252e-04 - acc: 1.0000 - val_loss: 0.3015 - val_acc: 0.9583\n",
            "Epoch 366/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.8416e-05 - acc: 1.0000 - val_loss: 0.3055 - val_acc: 0.9583\n",
            "Epoch 367/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.7750e-04 - acc: 0.9995 - val_loss: 0.3206 - val_acc: 0.9583\n",
            "Epoch 368/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.8512e-04 - acc: 1.0000 - val_loss: 0.2556 - val_acc: 0.9500\n",
            "Epoch 369/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.0656e-04 - acc: 1.0000 - val_loss: 0.2508 - val_acc: 0.9500\n",
            "Epoch 370/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.6429e-05 - acc: 1.0000 - val_loss: 0.2422 - val_acc: 0.9583\n",
            "Epoch 371/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.0056e-04 - acc: 1.0000 - val_loss: 0.2283 - val_acc: 0.9625\n",
            "Epoch 372/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.6594e-04 - acc: 1.0000 - val_loss: 0.2491 - val_acc: 0.9542\n",
            "Epoch 373/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.4759e-04 - acc: 1.0000 - val_loss: 0.2325 - val_acc: 0.9667\n",
            "Epoch 374/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.2592e-04 - acc: 1.0000 - val_loss: 0.2201 - val_acc: 0.9625\n",
            "Epoch 375/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.6103e-04 - acc: 0.9995 - val_loss: 0.2910 - val_acc: 0.9583\n",
            "Epoch 376/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.0255e-04 - acc: 1.0000 - val_loss: 0.2792 - val_acc: 0.9583\n",
            "Epoch 377/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.4947e-05 - acc: 1.0000 - val_loss: 0.2968 - val_acc: 0.9583\n",
            "Epoch 378/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.0600e-04 - acc: 1.0000 - val_loss: 0.3054 - val_acc: 0.9542\n",
            "Epoch 379/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.9389e-04 - acc: 1.0000 - val_loss: 0.2473 - val_acc: 0.9583\n",
            "Epoch 380/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.2324 - val_acc: 0.9583\n",
            "Epoch 381/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.8893e-04 - acc: 1.0000 - val_loss: 0.2192 - val_acc: 0.9542\n",
            "Epoch 382/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.0891e-04 - acc: 0.9995 - val_loss: 0.2196 - val_acc: 0.9583\n",
            "Epoch 383/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.7143e-04 - acc: 1.0000 - val_loss: 0.2303 - val_acc: 0.9583\n",
            "Epoch 384/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.5842e-04 - acc: 1.0000 - val_loss: 0.2304 - val_acc: 0.9583\n",
            "Epoch 385/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.8290e-05 - acc: 1.0000 - val_loss: 0.2360 - val_acc: 0.9583\n",
            "Epoch 386/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.4439e-05 - acc: 1.0000 - val_loss: 0.2760 - val_acc: 0.9625\n",
            "Epoch 387/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.6228e-04 - acc: 0.9995 - val_loss: 0.2344 - val_acc: 0.9625\n",
            "Epoch 388/500\n",
            "2160/2160 [==============================] - 7s 3ms/step - loss: 4.7479e-05 - acc: 1.0000 - val_loss: 0.2446 - val_acc: 0.9625\n",
            "Epoch 389/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.2433 - val_acc: 0.9625\n",
            "Epoch 390/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.1092e-04 - acc: 1.0000 - val_loss: 0.2426 - val_acc: 0.9625\n",
            "Epoch 391/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.2352e-04 - acc: 1.0000 - val_loss: 0.2322 - val_acc: 0.9667\n",
            "Epoch 392/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.2715e-04 - acc: 1.0000 - val_loss: 0.2358 - val_acc: 0.9667\n",
            "Epoch 393/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 8.2549e-05 - acc: 1.0000 - val_loss: 0.2457 - val_acc: 0.9667\n",
            "Epoch 394/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.3338e-04 - acc: 0.9995 - val_loss: 0.2567 - val_acc: 0.9625\n",
            "Epoch 395/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.6545e-04 - acc: 1.0000 - val_loss: 0.2600 - val_acc: 0.9625\n",
            "Epoch 396/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 9.6925e-05 - acc: 1.0000 - val_loss: 0.2527 - val_acc: 0.9625\n",
            "Epoch 397/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.7077e-04 - acc: 1.0000 - val_loss: 0.3206 - val_acc: 0.9500\n",
            "Epoch 398/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.2564e-05 - acc: 1.0000 - val_loss: 0.2987 - val_acc: 0.9583\n",
            "Epoch 399/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.0451e-04 - acc: 1.0000 - val_loss: 0.2508 - val_acc: 0.9583\n",
            "Epoch 400/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.8476e-04 - acc: 0.9995 - val_loss: 0.2520 - val_acc: 0.9542\n",
            "Epoch 401/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.6062e-05 - acc: 1.0000 - val_loss: 0.2538 - val_acc: 0.9542\n",
            "Epoch 402/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.2861e-05 - acc: 1.0000 - val_loss: 0.2421 - val_acc: 0.9542\n",
            "Epoch 403/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.6064e-05 - acc: 1.0000 - val_loss: 0.2447 - val_acc: 0.9542\n",
            "Epoch 404/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.0493e-04 - acc: 0.9995 - val_loss: 0.2477 - val_acc: 0.9625\n",
            "Epoch 405/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.6335e-04 - acc: 1.0000 - val_loss: 0.2278 - val_acc: 0.9583\n",
            "Epoch 406/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.3167e-04 - acc: 1.0000 - val_loss: 0.2220 - val_acc: 0.9583\n",
            "Epoch 407/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.1492e-05 - acc: 1.0000 - val_loss: 0.1939 - val_acc: 0.9625\n",
            "Epoch 408/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.8041e-05 - acc: 1.0000 - val_loss: 0.2047 - val_acc: 0.9542\n",
            "Epoch 409/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.7656e-04 - acc: 1.0000 - val_loss: 0.2397 - val_acc: 0.9583\n",
            "Epoch 410/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.6490e-05 - acc: 1.0000 - val_loss: 0.2410 - val_acc: 0.9542\n",
            "Epoch 411/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.4821e-05 - acc: 1.0000 - val_loss: 0.2546 - val_acc: 0.9542\n",
            "Epoch 412/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.9055e-04 - acc: 1.0000 - val_loss: 0.2780 - val_acc: 0.9583\n",
            "Epoch 413/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.9181e-04 - acc: 0.9995 - val_loss: 0.2911 - val_acc: 0.9542\n",
            "Epoch 414/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.9434e-04 - acc: 1.0000 - val_loss: 0.2279 - val_acc: 0.9583\n",
            "Epoch 415/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 8.4842e-05 - acc: 1.0000 - val_loss: 0.2257 - val_acc: 0.9583\n",
            "Epoch 416/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.7912e-04 - acc: 1.0000 - val_loss: 0.2072 - val_acc: 0.9625\n",
            "Epoch 417/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.6231e-04 - acc: 1.0000 - val_loss: 0.2227 - val_acc: 0.9625\n",
            "Epoch 418/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.7315e-05 - acc: 1.0000 - val_loss: 0.2213 - val_acc: 0.9583\n",
            "Epoch 419/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.2846e-04 - acc: 1.0000 - val_loss: 0.2141 - val_acc: 0.9667\n",
            "Epoch 420/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.5691e-05 - acc: 1.0000 - val_loss: 0.2140 - val_acc: 0.9625\n",
            "Epoch 421/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.5598e-05 - acc: 1.0000 - val_loss: 0.2244 - val_acc: 0.9625\n",
            "Epoch 422/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.2320e-05 - acc: 1.0000 - val_loss: 0.2147 - val_acc: 0.9625\n",
            "Epoch 423/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.3611e-04 - acc: 1.0000 - val_loss: 0.2671 - val_acc: 0.9542\n",
            "Epoch 424/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.1339e-05 - acc: 1.0000 - val_loss: 0.2869 - val_acc: 0.9500\n",
            "Epoch 425/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.5399e-04 - acc: 1.0000 - val_loss: 0.3001 - val_acc: 0.9542\n",
            "Epoch 426/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.1205e-04 - acc: 1.0000 - val_loss: 0.2554 - val_acc: 0.9542\n",
            "Epoch 427/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.0741e-04 - acc: 1.0000 - val_loss: 0.2445 - val_acc: 0.9583\n",
            "Epoch 428/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.5367e-05 - acc: 1.0000 - val_loss: 0.2484 - val_acc: 0.9583\n",
            "Epoch 429/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.3728e-05 - acc: 1.0000 - val_loss: 0.2479 - val_acc: 0.9583\n",
            "Epoch 430/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.2030e-05 - acc: 1.0000 - val_loss: 0.2377 - val_acc: 0.9583\n",
            "Epoch 431/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.5927e-04 - acc: 1.0000 - val_loss: 0.2429 - val_acc: 0.9583\n",
            "Epoch 432/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.6023e-04 - acc: 0.9995 - val_loss: 0.2761 - val_acc: 0.9542\n",
            "Epoch 433/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.3115e-05 - acc: 1.0000 - val_loss: 0.2808 - val_acc: 0.9542\n",
            "Epoch 434/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.5444e-05 - acc: 1.0000 - val_loss: 0.2665 - val_acc: 0.9583\n",
            "Epoch 435/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.1470e-04 - acc: 1.0000 - val_loss: 0.2617 - val_acc: 0.9583\n",
            "Epoch 436/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.0866e-04 - acc: 1.0000 - val_loss: 0.2815 - val_acc: 0.9583\n",
            "Epoch 437/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.0045e-05 - acc: 1.0000 - val_loss: 0.2657 - val_acc: 0.9583\n",
            "Epoch 438/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.1433e-04 - acc: 1.0000 - val_loss: 0.2015 - val_acc: 0.9542\n",
            "Epoch 439/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.3118e-04 - acc: 1.0000 - val_loss: 0.2261 - val_acc: 0.9583\n",
            "Epoch 440/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.1661e-05 - acc: 1.0000 - val_loss: 0.2243 - val_acc: 0.9583\n",
            "Epoch 441/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.4762e-05 - acc: 1.0000 - val_loss: 0.2214 - val_acc: 0.9583\n",
            "Epoch 442/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.1039e-04 - acc: 1.0000 - val_loss: 0.2599 - val_acc: 0.9500\n",
            "Epoch 443/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.0480e-05 - acc: 1.0000 - val_loss: 0.2495 - val_acc: 0.9542\n",
            "Epoch 444/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.8177e-05 - acc: 1.0000 - val_loss: 0.2350 - val_acc: 0.9542\n",
            "Epoch 445/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.4189e-05 - acc: 1.0000 - val_loss: 0.2047 - val_acc: 0.9542\n",
            "Epoch 446/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.9262e-05 - acc: 1.0000 - val_loss: 0.2211 - val_acc: 0.9583\n",
            "Epoch 447/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 8.2680e-05 - acc: 1.0000 - val_loss: 0.2310 - val_acc: 0.9500\n",
            "Epoch 448/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.5479e-05 - acc: 1.0000 - val_loss: 0.2122 - val_acc: 0.9542\n",
            "Epoch 449/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.9523e-05 - acc: 1.0000 - val_loss: 0.2244 - val_acc: 0.9542\n",
            "Epoch 450/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 8.6739e-05 - acc: 1.0000 - val_loss: 0.2377 - val_acc: 0.9500\n",
            "Epoch 451/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 9.1447e-05 - acc: 1.0000 - val_loss: 0.2469 - val_acc: 0.9542\n",
            "Epoch 452/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.3661e-05 - acc: 1.0000 - val_loss: 0.2271 - val_acc: 0.9542\n",
            "Epoch 453/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.2612e-05 - acc: 1.0000 - val_loss: 0.2289 - val_acc: 0.9542\n",
            "Epoch 454/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.3953e-04 - acc: 1.0000 - val_loss: 0.2508 - val_acc: 0.9583\n",
            "Epoch 455/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.3326e-04 - acc: 0.9995 - val_loss: 0.2319 - val_acc: 0.9542\n",
            "Epoch 456/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.6451e-04 - acc: 1.0000 - val_loss: 0.2335 - val_acc: 0.9542\n",
            "Epoch 457/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.0141e-05 - acc: 1.0000 - val_loss: 0.2311 - val_acc: 0.9583\n",
            "Epoch 458/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 9.9724e-05 - acc: 1.0000 - val_loss: 0.2443 - val_acc: 0.9542\n",
            "Epoch 459/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.7014e-05 - acc: 1.0000 - val_loss: 0.2551 - val_acc: 0.9542\n",
            "Epoch 460/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.7546e-05 - acc: 1.0000 - val_loss: 0.2573 - val_acc: 0.9583\n",
            "Epoch 461/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.0613e-04 - acc: 1.0000 - val_loss: 0.2156 - val_acc: 0.9625\n",
            "Epoch 462/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.8465e-05 - acc: 1.0000 - val_loss: 0.2157 - val_acc: 0.9583\n",
            "Epoch 463/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.0723e-05 - acc: 1.0000 - val_loss: 0.2068 - val_acc: 0.9542\n",
            "Epoch 464/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 6.2948e-05 - acc: 1.0000 - val_loss: 0.2185 - val_acc: 0.9542\n",
            "Epoch 465/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.9240e-05 - acc: 1.0000 - val_loss: 0.2184 - val_acc: 0.9625\n",
            "Epoch 466/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.4296e-05 - acc: 1.0000 - val_loss: 0.2192 - val_acc: 0.9625\n",
            "Epoch 467/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.2248e-05 - acc: 1.0000 - val_loss: 0.2112 - val_acc: 0.9625\n",
            "Epoch 468/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.3654e-05 - acc: 1.0000 - val_loss: 0.2051 - val_acc: 0.9625\n",
            "Epoch 469/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.9015e-05 - acc: 1.0000 - val_loss: 0.2156 - val_acc: 0.9583\n",
            "Epoch 470/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.9638e-05 - acc: 1.0000 - val_loss: 0.2253 - val_acc: 0.9625\n",
            "Epoch 471/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.3442e-05 - acc: 1.0000 - val_loss: 0.2211 - val_acc: 0.9625\n",
            "Epoch 472/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.9835e-05 - acc: 1.0000 - val_loss: 0.2482 - val_acc: 0.9583\n",
            "Epoch 473/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.9520e-05 - acc: 1.0000 - val_loss: 0.2304 - val_acc: 0.9625\n",
            "Epoch 474/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 9.5715e-05 - acc: 1.0000 - val_loss: 0.2204 - val_acc: 0.9625\n",
            "Epoch 475/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.6519e-05 - acc: 1.0000 - val_loss: 0.2185 - val_acc: 0.9625\n",
            "Epoch 476/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.7716e-04 - acc: 1.0000 - val_loss: 0.2169 - val_acc: 0.9583\n",
            "Epoch 477/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.3984e-05 - acc: 1.0000 - val_loss: 0.2173 - val_acc: 0.9625\n",
            "Epoch 478/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 7.7813e-05 - acc: 1.0000 - val_loss: 0.2144 - val_acc: 0.9583\n",
            "Epoch 479/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.1624e-04 - acc: 1.0000 - val_loss: 0.2032 - val_acc: 0.9583\n",
            "Epoch 480/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.9842e-05 - acc: 1.0000 - val_loss: 0.2039 - val_acc: 0.9583\n",
            "Epoch 481/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.1364e-05 - acc: 1.0000 - val_loss: 0.2035 - val_acc: 0.9583\n",
            "Epoch 482/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.8157e-04 - acc: 1.0000 - val_loss: 0.2030 - val_acc: 0.9583\n",
            "Epoch 483/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.9419e-05 - acc: 1.0000 - val_loss: 0.2138 - val_acc: 0.9542\n",
            "Epoch 484/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.9647e-05 - acc: 1.0000 - val_loss: 0.2134 - val_acc: 0.9542\n",
            "Epoch 485/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.9380e-04 - acc: 1.0000 - val_loss: 0.2836 - val_acc: 0.9542\n",
            "Epoch 486/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.5632e-04 - acc: 1.0000 - val_loss: 0.2900 - val_acc: 0.9542\n",
            "Epoch 487/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.5150e-05 - acc: 1.0000 - val_loss: 0.2914 - val_acc: 0.9542\n",
            "Epoch 488/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.4541e-05 - acc: 1.0000 - val_loss: 0.2859 - val_acc: 0.9542\n",
            "Epoch 489/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.0459e-05 - acc: 1.0000 - val_loss: 0.2875 - val_acc: 0.9542\n",
            "Epoch 490/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.1503e-04 - acc: 1.0000 - val_loss: 0.3048 - val_acc: 0.9542\n",
            "Epoch 491/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.2258e-05 - acc: 1.0000 - val_loss: 0.2576 - val_acc: 0.9583\n",
            "Epoch 492/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.9496e-05 - acc: 1.0000 - val_loss: 0.2723 - val_acc: 0.9542\n",
            "Epoch 493/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 2.4841e-05 - acc: 1.0000 - val_loss: 0.2720 - val_acc: 0.9417\n",
            "Epoch 494/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 1.9037e-04 - acc: 1.0000 - val_loss: 0.2849 - val_acc: 0.9542\n",
            "Epoch 495/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.4955e-05 - acc: 1.0000 - val_loss: 0.2761 - val_acc: 0.9542\n",
            "Epoch 496/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.2779e-05 - acc: 1.0000 - val_loss: 0.2631 - val_acc: 0.9583\n",
            "Epoch 497/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 8.3658e-05 - acc: 1.0000 - val_loss: 0.2774 - val_acc: 0.9542\n",
            "Epoch 498/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 4.9056e-05 - acc: 1.0000 - val_loss: 0.2926 - val_acc: 0.9542\n",
            "Epoch 499/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 3.6203e-05 - acc: 1.0000 - val_loss: 0.2809 - val_acc: 0.9542\n",
            "Epoch 500/500\n",
            "2160/2160 [==============================] - 6s 3ms/step - loss: 5.7478e-05 - acc: 1.0000 - val_loss: 0.2904 - val_acc: 0.9542\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f73f2ee28d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MksGQBpkP_Th",
        "colab_type": "code",
        "outputId": "90a6af8e-8eb7-4ef1-ac3e-dcb28e4ec56a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(600, 40, 11, 1)\n",
            "(600, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH1GiDOeP6l7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(\"X_train\",X_train.reshape(-1,40*11))\n",
        "np.save(\"y_train\",np.array([np.argmax(u) for u in y_train]))\n",
        "\n",
        "np.save(\"y_test\",np.array([np.argmax(u) for u in y_test]))\n",
        "\n",
        "np.save(\"X_test\",X_test.reshape(-1,40*11))\n",
        "X = np.concatenate((X_train, X_test))\n",
        "y=np.concatenate((y_train, y_test))\n",
        "np.save(\"X\",X)\n",
        "np.save(\"y\",y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBEommjpR39h",
        "colab_type": "code",
        "outputId": "31b20cfb-3114-4de7-b6bf-10ec143b3032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "import random\n",
        "import math, sys\n",
        "import sklearn\n",
        "import numexpr as ne\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.kernel_approximation import Nystroem, RBFSampler\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.cluster import KMeans\n",
        "import datetime\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "np.set_printoptions(precision=4, suppress=True, threshold=1000, linewidth=500)\n",
        "ne.set_num_threads(32)\n",
        "random.seed(1)\n",
        "\n",
        "def zca_whitening(inputs):\n",
        "    inputs -= np.mean(inputs, axis=0)\n",
        "    sigma = np.dot(inputs.T, inputs)/inputs.shape[0]\n",
        "    U,S,V = np.linalg.svd(sigma)\n",
        "    epsilon = 0.1\n",
        "    ZCAMatrix = np.dot(np.dot(U, np.diag(1.0/np.sqrt(S + epsilon))), U.T).astype(np.float32)\n",
        "\n",
        "    i = 0\n",
        "    while i < inputs.shape[0]:\n",
        "        next_i = min(inputs.shape[0], i+100000)\n",
        "        inputs[i:next_i] = np.dot(inputs[i:next_i], ZCAMatrix.T)\n",
        "        i = next_i\n",
        "\n",
        "    return inputs\n",
        "\n",
        "class NystroemTransformer:\n",
        "    reference_matrix = 0\n",
        "    transform_matrix = 0\n",
        "    n_components = 0\n",
        "    gamma = 0\n",
        "\n",
        "    def __init__(self, gamma, n_components):\n",
        "        self.n_components = n_components\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def fit(self, X):\n",
        "        n = X.shape[0]\n",
        "        index = np.random.randint(0, n, self.n_components)\n",
        "        self.reference_matrix = np.copy(X[index])\n",
        "        kernel_matrix = rbf_kernel_matrix(gamma=self.gamma, X=self.reference_matrix, Y=self.reference_matrix)\n",
        "        (U, s, V) = LA.svd(kernel_matrix)\n",
        "        self.transform_matrix = np.dot(U, np.dot(np.diag(1.0/np.sqrt(s)), V))\n",
        "\n",
        "    def transform(self, Y):\n",
        "        kernel_matrix = rbf_kernel_matrix(gamma=self.gamma, X=self.reference_matrix, Y=Y)\n",
        "        output = (np.dot(self.transform_matrix, kernel_matrix)).T\n",
        "        return output\n",
        "\n",
        "class RandomFourierTransformer:\n",
        "    transform_matrix = 0\n",
        "    transform_bias = 0\n",
        "    n_components = 0\n",
        "    gamma = 0\n",
        "\n",
        "    def __init__(self, gamma, n_components):\n",
        "        self.n_components = n_components\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def fit(self, X):\n",
        "        d = X.shape[1]\n",
        "        self.transform_matrix = np.random.normal(loc=0, scale=math.sqrt(2*self.gamma), size=(d, self.n_components)).astype(np.float32)\n",
        "        self.transform_bias = (np.random.rand(1, self.n_components) * 2 * math.pi).astype(np.float32)\n",
        "\n",
        "    def transform(self, Y):\n",
        "        ny = Y.shape[0]\n",
        "        angle = np.dot(Y, self.transform_matrix)\n",
        "        bias = self.transform_bias\n",
        "        factor = np.float32(math.sqrt(2.0 / self.n_components))\n",
        "        return ne.evaluate(\"factor*cos(angle+bias)\")\n",
        "\n",
        "def rbf_kernel_matrix(gamma, X, Y):\n",
        "    nx = X.shape[0]\n",
        "    ny = Y.shape[0]\n",
        "    X2 = np.dot(np.sum(np.square(X), axis=1).reshape((nx, 1)), np.ones((1,ny), dtype=np.float32))\n",
        "    Y2 = np.dot(np.ones((nx,1), dtype=np.float32), np.sum(np.square(Y), axis=1).reshape((1, ny)))\n",
        "    XY = np.dot(X, Y.T)\n",
        "    return ne.evaluate(\"exp(gamma*(2*XY-X2-Y2))\")\n",
        "\n",
        "def tprint(s):\n",
        "    tm_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time()))\n",
        "    print(tm_str + \":  \" + str(s))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def safe_exp(X):\n",
        "    return np.exp(np.maximum(np.minimum(X, 20), -20))\n",
        "\n",
        "def normalize_vec(v):\n",
        "    norm = LA.norm(v)\n",
        "    if norm > 0:\n",
        "        return v / norm\n",
        "    else:\n",
        "        return v\n",
        "\n",
        "def euclidean_proj_simplex(v, s=1):\n",
        "    \"\"\" Compute the Euclidean projection on a positive simplex\n",
        "    Solves the optimisation problem (using the algorithm from [1]):\n",
        "        min_w 0.5 * || w - v ||_2^2 , s.t. \\sum_i w_i = s, w_i >= 0\n",
        "    Parameters\n",
        "    ----------\n",
        "    v: (n,) numpy array,\n",
        "       n-dimensional vector to project\n",
        "    s: int, optional, default: 1,\n",
        "       radius of the simplex\n",
        "    Returns\n",
        "    -------\n",
        "    w: (n,) numpy array,\n",
        "       Euclidean projection of v on the simplex\n",
        "    Notes\n",
        "    -----\n",
        "    The complexity of this algorithm is in O(n log(n)) as it involves sorting v.\n",
        "    Better alternatives exist for high-dimensional sparse vectors (cf. [1])\n",
        "    However, this implementation still easily scales to millions of dimensions.\n",
        "    References\n",
        "    ----------\n",
        "    [1] Efficient Projections onto the .1-Ball for Learning in High Dimensions\n",
        "        John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.\n",
        "        International Conference on Machine Learning (ICML 2008)\n",
        "        http://www.cs.berkeley.edu/~jduchi/projects/DuchiSiShCh08.pdf\n",
        "    \"\"\"\n",
        "    assert s > 0, \"Radius s must be strictly positive (%d <= 0)\" % s\n",
        "    n, = v.shape  # will raise ValueError if v is not 1-D\n",
        "    # check if we are already on the simplex\n",
        "    if v.sum() == s and np.alltrue(v >= 0):\n",
        "        # best projection: itself!\n",
        "        return v\n",
        "    # get the array of cumulative sums of a sorted (decreasing) copy of v\n",
        "    u = np.sort(v)[::-1]\n",
        "    cssv = np.cumsum(u)\n",
        "    # get the number of > 0 components of the optimal solution\n",
        "    rho = np.nonzero(u * np.arange(1, n+1) > (cssv - s))[0][-1]\n",
        "    # compute the Lagrange multiplier associated to the simplex constraint\n",
        "    theta = (cssv[rho] - s) / (rho + 1.0)\n",
        "    # compute the projection by thresholding v using theta\n",
        "    w = (v - theta).clip(min=0)\n",
        "    return w\n",
        "\n",
        "def euclidean_proj_l1ball(v, s=1):\n",
        "    \"\"\" Compute the Euclidean projection on a L1-ball\n",
        "    Solves the optimisation problem (using the algorithm from [1]):\n",
        "        min_w 0.5 * || w - v ||_2^2 , s.t. || w ||_1 <= s\n",
        "    Parameters\n",
        "    ----------\n",
        "    v: (n,) numpy array,\n",
        "       n-dimensional vector to project\n",
        "    s: int, optional, default: 1,\n",
        "       radius of the L1-ball\n",
        "    Returns\n",
        "    -------\n",
        "    w: (n,) numpy array,\n",
        "       Euclidean projection of v on the L1-ball of radius s\n",
        "    Notes\n",
        "    -----\n",
        "    Solves the problem by a reduction to the positive simplex case\n",
        "    See also\n",
        "    --------\n",
        "    euclidean_proj_simplex\n",
        "    \"\"\"\n",
        "    assert s > 0, \"Radius s must be strictly positive (%d <= 0)\" % s\n",
        "    n, = v.shape  # will raise ValueError if v is not 1-D\n",
        "    # compute the vector of absolute values\n",
        "    u = np.abs(v)\n",
        "    # check if v is already a solution\n",
        "    if u.sum() <= s:\n",
        "        # L1-norm is <= s\n",
        "        return v\n",
        "    # v is not already a solution: optimum lies on the boundary (norm == s)\n",
        "    # project *u* on the simplex\n",
        "    w = euclidean_proj_simplex(u, s=s)\n",
        "    # compute the solution to the original problem on v\n",
        "    w *= np.sign(v)\n",
        "    return w\n",
        "\n",
        "def get_pixel_vector(center_x, center_y, radius, image_width):\n",
        "    size = int(radius * 2 + 1)\n",
        "    vector = np.zeros(size*size, dtype=int)\n",
        "    for y in range(0, size):\n",
        "        for x in range(0, size):\n",
        "            index = (center_x+x-radius) + (center_y+y-radius) * image_width\n",
        "            vector[x+y*size] = index\n",
        "    return vector\n",
        "\n",
        "def get_pixel_index_matrix(center_x, center_y, radius, image_width):\n",
        "    size = (radius * 2 + 1)*(radius * 2 + 1)\n",
        "    matrix = np.zeros((size, size), dtype=int)\n",
        "    for y in range(0, 2*radius+1):\n",
        "        for x in range(0, 2*radius+1):\n",
        "            cursor_x = center_x+x-radius\n",
        "            cursor_y = center_y+y-radius\n",
        "            matrix[x+y*(2*radius+1)] = get_pixel_vector(cursor_x, cursor_y, radius, image_width)\n",
        "    return matrix\n",
        "\n",
        "def project_to_low_rank(A, rank, d1, d2):\n",
        "    A = np.reshape(A, (9*d1, d2))\n",
        "    (U, s, V) = LA.svd(A, full_matrices=False)\n",
        "    s[rank:] *= 0\n",
        "    return np.reshape(np.dot(U, np.dot(np.diag(s), V)), (9, d1*d2)), U, s, V\n",
        "\n",
        "def project_to_trace_norm(A, trace_norm, d1, d2):\n",
        "    A = np.reshape(A, (9*d1, d2))\n",
        "    (U, s, V) = LA.svd(A, full_matrices=False)\n",
        "    s = euclidean_proj_l1ball(s, s=trace_norm)\n",
        "    return np.reshape(np.dot(U, np.dot(np.diag(s), V)), (9, d1*d2)), U, s, V\n",
        "\n",
        "def project_to_trace_regularizer(A, trace_regularizer, d1, d2):\n",
        "    A = np.reshape(A, (9*d1, d2))\n",
        "    (U, s, V) = LA.svd(A, full_matrices=False)\n",
        "    s = np.maximum(s-trace_regularizer, 0)\n",
        "    return np.reshape(np.dot(U, np.dot(np.diag(s), V)), (9, d1*d2)), U, s, V\n",
        "\n",
        "def evaluate_classifier(X_train, X_test, Y_train, Y_test, A):\n",
        "    n_train = X_train.shape[0]\n",
        "    n_test = X_test.shape[0]\n",
        "    eXAY = np.exp(np.sum((np.dot(X_train, A.T)) * Y_train[:,0:9], axis=1)) # batch_size-9\n",
        "    eXA_sum = np.sum(np.exp(np.dot(X_train, A.T)), axis=1) + 1\n",
        "    loss = - np.average(np.log(eXAY/eXA_sum))\n",
        "\n",
        "    predict_train = np.concatenate((np.dot(X_train, A.T), np.zeros((n_train, 1), dtype=np.float32)), axis=1)\n",
        "    predict_test = np.concatenate((np.dot(X_test, A.T), np.zeros((n_test, 1), dtype=np.float32)), axis=1)\n",
        "\n",
        "    error_train = np.average(np.argmax(predict_train, axis=1) != np.argmax(Y_train, axis=1).astype(int))\n",
        "    error_test = np.average(np.argmax(predict_test, axis=1) != np.argmax(Y_test, axis=1).astype(int))\n",
        "\n",
        "    return loss, error_train, error_test\n",
        "\n",
        "def random_crop(X, d1, d2, radio):\n",
        "    n = X.shape[0]\n",
        "    size = int(math.sqrt(d1))\n",
        "    cropped_size = int(size*radio)\n",
        "    X = X.reshape((n, size, size, d2))\n",
        "    X_cropped = np.zeros((n, cropped_size, cropped_size, d2), dtype=np.float32)\n",
        "    for i in range(n):\n",
        "        y = np.random.randint(size - cropped_size + 1)\n",
        "        x = np.random.randint(size - cropped_size + 1)\n",
        "        X_cropped[i] = X[i, y:y+cropped_size, x:x+cropped_size]\n",
        "    return X_cropped.reshape((n, cropped_size*cropped_size*d2))\n",
        "\n",
        "def central_crop(X, d1, d2, radio):\n",
        "    n = X.shape[0]\n",
        "    size = int(math.sqrt(d1))\n",
        "    cropped_size = int(size*radio)\n",
        "    X = X.reshape((n, size, size, d2))\n",
        "    begin = int((size-cropped_size)/2)\n",
        "    return X[:,begin:begin+cropped_size, begin:begin+cropped_size].reshape((n, cropped_size*cropped_size*d2))\n",
        "\n",
        "def low_rank_matrix_regression(X_train, Y_train, X_test, Y_test, d1, d2, reg, n_iter, learning_rate, ratio):\n",
        "    n_train = X_train.shape[0]\n",
        "    cropped_d1 = int(d1*ratio*ratio)\n",
        "    A = np.zeros((9, cropped_d1*d2), dtype=np.float32) # 9-(d1*d2)\n",
        "    A_sum = np.zeros((9, cropped_d1*d2), dtype=np.float32) # 9-(d1*d2)\n",
        "    computation_time = 0\n",
        "\n",
        "    for t in range(n_iter):\n",
        "        mini_batch_size = 50\n",
        "        batch_size = 10\n",
        "\n",
        "        start = time.time()\n",
        "        for i in range(0, batch_size):\n",
        "            index = np.random.randint(0, n_train, mini_batch_size)\n",
        "            X_sample = random_crop(X_train[index], d1, d2, ratio) # batch-(d1*d2)\n",
        "            Y_sample = Y_train[index, 0:9] # batch-9\n",
        "\n",
        "            # stochastic gradient descent\n",
        "            XA = np.dot(X_sample, A.T)\n",
        "            eXA = ne.evaluate(\"exp(XA)\")\n",
        "            # eXA = np.exp(XA)\n",
        "            eXA_sum = np.sum(eXA, axis=1).reshape((mini_batch_size, 1)) + 1\n",
        "            diff = ne.evaluate(\"eXA/eXA_sum - Y_sample\")\n",
        "            grad_A = np.dot(diff.T, X_sample) / mini_batch_size\n",
        "            # grad_A = np.dot((eXA/eXA_sum - Y_sample).T, X_sample) / mini_batch_size\n",
        "            A -= learning_rate * grad_A\n",
        "\n",
        "        # projection to trace norm ball\n",
        "        A, U, s, V = project_to_trace_norm(A, reg, cropped_d1, d2)\n",
        "        end = time.time()\n",
        "        computation_time += end - start\n",
        "\n",
        "        A_sum += A\n",
        "        if (t+1) % 250 == 0:\n",
        "            dim = np.sum(s[0:25]) / np.sum(s)\n",
        "            A_avg = A_sum / 250\n",
        "            loss, error_train, error_test = evaluate_classifier(central_crop(X_train, d1, d2, ratio),\n",
        "                                                                central_crop(X_test, d1, d2, ratio), Y_train, Y_test, A_avg)\n",
        "            A_sum = np.zeros((9, cropped_d1*d2), dtype=np.float32)\n",
        "\n",
        "            # debug\n",
        "            tprint(\"iter \" + str(t+1) + \": loss=\" + str(loss) + \", train=\" + str(error_train) + \", test=\" + str(error_test) + \", dim=\" + str(dim))\n",
        "            # print(str(computation_time) + \"\\t\" + str(error_test))\n",
        "\n",
        "    A_avg, U, s, V = project_to_trace_norm(np.reshape(A_avg, (9*cropped_d1, d2)), reg, cropped_d1, d2)\n",
        "    dim = min(np.sum((s > 0).astype(int)), 25)\n",
        "    return V[0:dim]\n",
        "\n",
        "def transform_and_pooling(patch, transformer, selected_group_size, gamma, nystrom_dim,\n",
        "                                  patch_per_side, pooling_size, pooling_stride):\n",
        "    n = patch.shape[0]\n",
        "    patch_per_image = patch.shape[1]\n",
        "    selected_channel_num = patch.shape[2]\n",
        "    pixel_per_patch = patch.shape[3]\n",
        "    group_num = len(selected_group_size)\n",
        "    feature_dim = group_num * nystrom_dim\n",
        "\n",
        "    # construct Nystroem transformer\n",
        "    patch = patch.reshape((n*patch_per_image, selected_channel_num, pixel_per_patch))\n",
        "    psi = np.zeros((n*patch_per_image, group_num, nystrom_dim), dtype=np.float32)\n",
        "    if transformer[0] == 0:\n",
        "        transformer = np.empty(group_num, dtype=object)\n",
        "        sum = 0\n",
        "        for i in range(group_num):\n",
        "            # transformer[i] = NystroemTransformer(gamma=gamma, n_components=nystrom_dim)\n",
        "            transformer[i] = RandomFourierTransformer(gamma=gamma, n_components=nystrom_dim)\n",
        "            sub_patch = patch[:, sum:sum+selected_group_size[i]].reshape((n*patch_per_image, selected_group_size[i]*pixel_per_patch)) / math.sqrt(selected_group_size[i])\n",
        "\n",
        "            transformer[i].fit(X=sub_patch)\n",
        "            sum += selected_group_size[i]\n",
        "\n",
        "    # Nystrom transformation\n",
        "    sum = 0\n",
        "    for i in range(group_num):\n",
        "        sub_patch = patch[:, sum:sum+selected_group_size[i]].reshape((n*patch_per_image, selected_group_size[i]*pixel_per_patch)) / math.sqrt(selected_group_size[i])\n",
        "        psi[:, i] = transformer[i].transform(Y=sub_patch)\n",
        "        sum += selected_group_size[i]\n",
        "    psi = psi.reshape((n, patch_per_image, feature_dim))\n",
        "\n",
        "    # pooling\n",
        "    pooling_per_side = int(patch_per_side/pooling_stride)\n",
        "    pooling_per_image = pooling_per_side * pooling_per_side\n",
        "    psi_pooling = np.zeros((n, pooling_per_image, feature_dim), dtype=np.float32)\n",
        "\n",
        "    for pool_y in range(0, pooling_per_side):\n",
        "        range_y = np.array(range(pool_y*pooling_stride, min(pool_y*pooling_stride+pooling_size, patch_per_side)))\n",
        "        for pool_x in range(0, pooling_per_side):\n",
        "            range_x = np.array(range(pool_x*pooling_stride, min(pool_x*pooling_stride+pooling_size, patch_per_side)))\n",
        "            pooling_id = pool_x + pool_y * pooling_per_side\n",
        "            index = []\n",
        "            for y in range_y:\n",
        "                for x in range_x:\n",
        "                    index.append(x + y*patch_per_side)\n",
        "            psi_pooling[:, pooling_id] = np.average(psi[:, np.array(index)], axis=1)\n",
        "\n",
        "    return psi_pooling, transformer\n",
        "\n",
        "def generate_next_layer(input_file, output_file, label_file, n_train, n_test,\n",
        "                        patch_radius=2,\n",
        "                        nystrom_dim=200,\n",
        "                        pooling_size=2,\n",
        "                        pooling_stride=2,\n",
        "                        gamma=2,\n",
        "                        regularization_param=100,\n",
        "                        learning_rate=0.2,\n",
        "                        crop_ratio=1,\n",
        "                        n_iter=5000,\n",
        "                        chunk_size=5000,\n",
        "                        max_channel=16,\n",
        "                        generate_new_feature = True\n",
        "                        ):\n",
        "\n",
        "    tprint(\"read from \" + input_file)\n",
        "   \n",
        "   \n",
        "    if input_file.endswith(\".image\"):\n",
        "       \n",
        "       \n",
        "        X_train = np.load(\"X_train.npy\")\n",
        "        X_train=X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "\n",
        "        X_test = np.load(\"X_test.npy\")\n",
        "        X_test=X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "        \n",
        "        X_raw = np.concatenate((X_train, X_test))\n",
        "        label = np.concatenate((np.load(\"y_train.npy\"), np.load(\"y_test.npy\")))\n",
        "    else:\n",
        "        X_raw = np.load(open(input_file + \".npy\", \"rb\"))\n",
        "        X_raw = X_raw[0:n_train+n_test]\n",
        "        label = np.concatenate((np.load(\"y_train.npy\"), np.load(\"y_test.npy\")))\n",
        "    n = n_train + n_test\n",
        "\n",
        "    # detecting image parameters\n",
        "    pixel_per_image = X_raw.shape[2]\n",
        "    pixel_per_side = int(math.sqrt(pixel_per_image))\n",
        "    patch_per_side = int(pixel_per_side - 2 * patch_radius)\n",
        "    patch_per_image = patch_per_side * patch_per_side\n",
        "    patch_size = patch_radius * 2 + 1\n",
        "    pixel_per_patch = patch_size * patch_size\n",
        "    pooling_per_side = int(patch_per_side/pooling_stride)\n",
        "    pooling_per_image = pooling_per_side * pooling_per_side\n",
        "    tprint(\"Raw size = \" + str(X_raw.shape))\n",
        "\n",
        "    n_channel = min(max_channel, X_raw.shape[1])\n",
        "    selected_channel_list = range(0, n_channel)\n",
        "    selected_group_size = [n_channel]\n",
        "    tprint(selected_channel_list)\n",
        "    tprint(selected_group_size)\n",
        "    feature_dim = len(selected_group_size)*nystrom_dim\n",
        "\n",
        "    if generate_new_feature == True:\n",
        "        # construct patches\n",
        "        tprint(\"Construct patches...\")\n",
        "        patch = np.zeros((n, patch_per_image, len(selected_channel_list), pixel_per_patch), dtype=np.float32)\n",
        "        for y in range(0, patch_per_side):\n",
        "            for x in range(0, patch_per_side):\n",
        "                for i in selected_channel_list:\n",
        "                    indices = get_pixel_vector(x + patch_radius, y + patch_radius, patch_radius, pixel_per_side)\n",
        "                    patch_id = x + y * patch_per_side\n",
        "                    patch[:, patch_id, i] = X_raw[:, selected_channel_list[i], indices]\n",
        "\n",
        "        tprint(\"Patch size = \" + str(patch.shape))\n",
        "\n",
        "        # local contrast normalization and ZCA whitening\n",
        "        tprint('local contrast normalization and ZCA whitening...')\n",
        "        patch = patch.reshape((n*patch_per_image, n_channel*pixel_per_patch))\n",
        "        patch -= np.mean(patch, axis=1).reshape((patch.shape[0], 1))\n",
        "        patch /= LA.norm(patch, axis=1).reshape((patch.shape[0], 1)) + 0.1\n",
        "        patch = zca_whitening(patch)\n",
        "        patch = patch.reshape((n, patch_per_image, n_channel, pixel_per_patch))\n",
        "\n",
        "        # create features\n",
        "        tprint(\"Create features...\")\n",
        "        transformer = [0]\n",
        "        base = 0\n",
        "        X_reduced = np.zeros((n, pooling_per_image, feature_dim), dtype=np.float32)\n",
        "        while base < n:\n",
        "            tprint (\"  sample id \" + str(base) + \"-\" + str(min(n, base+chunk_size)))\n",
        "            X_reduced[base:min(n, base+chunk_size)], transformer = transform_and_pooling(patch=patch[base:min(n, base+chunk_size)],\n",
        "                    transformer=transformer, selected_group_size=selected_group_size, gamma=gamma,\n",
        "                    nystrom_dim=nystrom_dim, patch_per_side=patch_per_side, pooling_size=pooling_size, pooling_stride=pooling_stride)\n",
        "            base = min(n, base+chunk_size)\n",
        "\n",
        "        # normalization\n",
        "        X_reduced = X_reduced.reshape((n*pooling_per_image, feature_dim))\n",
        "        X_reduced -= np.mean(X_reduced, axis=0)\n",
        "        X_reduced /= LA.norm(X_reduced) / math.sqrt(n*pooling_per_image)\n",
        "        X_reduced = X_reduced.reshape((n, pooling_per_image*feature_dim))\n",
        "\n",
        "        # tprint(\"Saving features to hard disk...\")\n",
        "        # np.save(input_file+\".tmp\", X_reduced)\n",
        "\n",
        "    else:\n",
        "        tprint(\"Loading features from hard disk...\")\n",
        "        X_reduced = np.load(open(input_file + \".tmp.npy\", \"rb\"))\n",
        "\n",
        "    # Learning_filters\n",
        "    tprint(\"Training...\")\n",
        "    binary_label = label_binarize(label, classes=range(0, 10))\n",
        "    filter = low_rank_matrix_regression(X_train=X_reduced[0:n_train], Y_train=binary_label[0:n_train], X_test=X_reduced[n_train:],\n",
        "                        Y_test=binary_label[n_train:], d1=pooling_per_image, d2=feature_dim,\n",
        "                        n_iter=n_iter, reg=regularization_param, learning_rate=learning_rate, ratio=crop_ratio)\n",
        "\n",
        "    filter_dim = filter.shape[0]\n",
        "    tprint(\"Apply filters...\")\n",
        "    output = np.dot(X_reduced.reshape((n*pooling_per_image, feature_dim)), filter.T)\n",
        "    output = np.reshape(output, (n, pooling_per_image, filter_dim))\n",
        "    output = np.transpose(output, (0, 2, 1))\n",
        "\n",
        "    tprint(\"feature dimension = \" + str(output[0].size))\n",
        "    tprint(\"save to \" + output_file)\n",
        "    np.save(output_file, output)\n",
        "\n",
        "name = 'mnist_basic'\n",
        "tprint(\"name = \" + name)\n",
        "tprint(\"==========\")\n",
        "global_n_train = len(np.load(\"X_train.npy\"))\n",
        "global_n_test = len(np.load(\"X_test.npy\"))\n",
        "\n",
        "generate_next_layer(n_train=global_n_train, n_test=global_n_test, input_file=name+\".image\", output_file=name+\".feature1\", label_file=name+\".label\",\n",
        "                    chunk_size=100, gamma=0.2, nystrom_dim=500, regularization_param=200,\n",
        "                    learning_rate=0.2, n_iter=1000, generate_new_feature=True)\n",
        "\n",
        "generate_next_layer(n_train=global_n_train, n_test=global_n_test, input_file=name+\".feature1\", output_file=name+\".feature2\", label_file=name+\".label\",\n",
        "                    chunk_size=100, gamma=2, nystrom_dim=1000, regularization_param=500,\n",
        "                    learning_rate=1, n_iter=2000, generate_new_feature=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18:46:49:  name = mnist_basic\n",
            "18:46:49:  ==========\n",
            "18:46:49:  read from mnist_basic.image\n",
            "18:46:49:  Raw size = (3000, 1, 440)\n",
            "18:46:49:  range(0, 1)\n",
            "18:46:49:  [1]\n",
            "18:46:49:  Construct patches...\n",
            "18:46:50:  Patch size = (3000, 256, 1, 25)\n",
            "18:46:50:  local contrast normalization and ZCA whitening...\n",
            "18:46:50:  Create features...\n",
            "18:46:50:    sample id 0-100\n",
            "18:46:51:    sample id 100-200\n",
            "18:46:51:    sample id 200-300\n",
            "18:46:51:    sample id 300-400\n",
            "18:46:51:    sample id 400-500\n",
            "18:46:52:    sample id 500-600\n",
            "18:46:52:    sample id 600-700\n",
            "18:46:52:    sample id 700-800\n",
            "18:46:52:    sample id 800-900\n",
            "18:46:53:    sample id 900-1000\n",
            "18:46:53:    sample id 1000-1100\n",
            "18:46:53:    sample id 1100-1200\n",
            "18:46:53:    sample id 1200-1300\n",
            "18:46:53:    sample id 1300-1400\n",
            "18:46:54:    sample id 1400-1500\n",
            "18:46:54:    sample id 1500-1600\n",
            "18:46:54:    sample id 1600-1700\n",
            "18:46:54:    sample id 1700-1800\n",
            "18:46:55:    sample id 1800-1900\n",
            "18:46:55:    sample id 1900-2000\n",
            "18:46:55:    sample id 2000-2100\n",
            "18:46:55:    sample id 2100-2200\n",
            "18:46:56:    sample id 2200-2300\n",
            "18:46:56:    sample id 2300-2400\n",
            "18:46:56:    sample id 2400-2500\n",
            "18:46:56:    sample id 2500-2600\n",
            "18:46:57:    sample id 2600-2700\n",
            "18:46:57:    sample id 2700-2800\n",
            "18:46:57:    sample id 2800-2900\n",
            "18:46:57:    sample id 2900-3000\n",
            "18:46:58:  Training...\n",
            "18:48:07:  iter 250: loss=0.17020460451295275, train=0.00125, test=0.08833333333333333, dim=0.33063385\n",
            "18:49:16:  iter 500: loss=0.06768854632392002, train=0.0, test=0.07333333333333333, dim=0.39478612\n",
            "18:50:24:  iter 750: loss=0.05012117097971463, train=0.0, test=0.06833333333333333, dim=0.4365586\n",
            "18:51:31:  iter 1000: loss=0.04152227786356683, train=0.0, test=0.06666666666666667, dim=0.46807063\n",
            "18:51:31:  Apply filters...\n",
            "18:51:31:  feature dimension = 1600\n",
            "18:51:31:  save to mnist_basic.feature1\n",
            "18:51:32:  read from mnist_basic.feature1\n",
            "18:51:32:  Raw size = (3000, 25, 64)\n",
            "18:51:32:  range(0, 16)\n",
            "18:51:32:  [16]\n",
            "18:51:32:  Construct patches...\n",
            "18:51:32:  Patch size = (3000, 16, 16, 25)\n",
            "18:51:32:  local contrast normalization and ZCA whitening...\n",
            "18:51:33:  Create features...\n",
            "18:51:33:    sample id 0-100\n",
            "18:51:33:    sample id 100-200\n",
            "18:51:33:    sample id 200-300\n",
            "18:51:33:    sample id 300-400\n",
            "18:51:33:    sample id 400-500\n",
            "18:51:33:    sample id 500-600\n",
            "18:51:33:    sample id 600-700\n",
            "18:51:33:    sample id 700-800\n",
            "18:51:33:    sample id 800-900\n",
            "18:51:33:    sample id 900-1000\n",
            "18:51:33:    sample id 1000-1100\n",
            "18:51:33:    sample id 1100-1200\n",
            "18:51:33:    sample id 1200-1300\n",
            "18:51:33:    sample id 1300-1400\n",
            "18:51:33:    sample id 1400-1500\n",
            "18:51:34:    sample id 1500-1600\n",
            "18:51:34:    sample id 1600-1700\n",
            "18:51:34:    sample id 1700-1800\n",
            "18:51:34:    sample id 1800-1900\n",
            "18:51:34:    sample id 1900-2000\n",
            "18:51:34:    sample id 2000-2100\n",
            "18:51:34:    sample id 2100-2200\n",
            "18:51:34:    sample id 2200-2300\n",
            "18:51:34:    sample id 2300-2400\n",
            "18:51:34:    sample id 2400-2500\n",
            "18:51:34:    sample id 2500-2600\n",
            "18:51:34:    sample id 2600-2700\n",
            "18:51:34:    sample id 2700-2800\n",
            "18:51:34:    sample id 2800-2900\n",
            "18:51:34:    sample id 2900-3000\n",
            "18:51:34:  Training...\n",
            "18:51:43:  iter 250: loss=0.4383205158244787, train=0.034583333333333334, test=0.17, dim=0.7944608\n",
            "18:51:51:  iter 500: loss=0.14528187564116307, train=0.002916666666666667, test=0.13166666666666665, dim=0.7905037\n",
            "18:51:59:  iter 750: loss=0.0851385766543708, train=0.0004166666666666667, test=0.11333333333333333, dim=0.78928286\n",
            "18:52:08:  iter 1000: loss=0.059609144939889114, train=0.0, test=0.11333333333333333, dim=0.7885122\n",
            "18:52:16:  iter 1250: loss=0.04596593377366319, train=0.0, test=0.11333333333333333, dim=0.78938776\n",
            "18:52:24:  iter 1500: loss=0.042155287748438894, train=0.0, test=0.11166666666666666, dim=0.79393\n",
            "18:52:32:  iter 1750: loss=0.0410563206101031, train=0.0, test=0.11166666666666666, dim=0.7982989\n",
            "18:52:41:  iter 2000: loss=0.04015855235301362, train=0.0, test=0.11166666666666666, dim=0.80257565\n",
            "18:52:41:  Apply filters...\n",
            "18:52:41:  feature dimension = 100\n",
            "18:52:41:  save to mnist_basic.feature2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xflTzN7Wf5X",
        "colab_type": "code",
        "outputId": "8a4cc512-83ff-48d1-faf0-9ff44b0bd059",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import validation_curve\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "classifier = SVC(kernel='rbf',probability=True)\n",
        "\n",
        "\n",
        "# Split data into train and test subsets\n",
        "\n",
        "\n",
        "# We learn the digits on the first half of the digits\n",
        "classifier.fit(X_train.reshape(-1,40*11), np.array([np.argmax(u) for u in y_train]))\n",
        "print(classifier.score(X_test.reshape(-1,40*11), np.array([np.argmax(u) for u in y_test])))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9116666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOtlQsakj0-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
        "    \"\"\"\n",
        "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
        "    and predictions. \n",
        "    Input: predictions (N, k) ndarray\n",
        "           targets (N, k) ndarray        \n",
        "    Returns: scalar\n",
        "    \"\"\"\n",
        "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
        "    N = predictions.shape[0]\n",
        "    ce = -np.sum(targets*np.log(predictions+1e-9))/N\n",
        "    return ce\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNw5GcqdjMUL",
        "colab_type": "code",
        "outputId": "8e661cca-8b71-4b26-e5e1-92363a8c1f4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cross_entropy(classifier.predict_proba(X_test.reshape(-1,40*11)), y_test, epsilon=1e-12)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2509382589112557"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}